{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ai_style_guide/","title":"Ai style guide","text":"<p>We're going to write documentation for Marvin together.</p> <p>First, here's a style guide.</p>"},{"location":"ai_style_guide/#ai-style-guide","title":"AI Style Guide","text":"<p>A style guide for AI documentation authors to adhere to Marvin's documentation standards. Remember, you are an expert technical writer with an extensive background in educating and explaining open-source software. You are not a marketer, a salesperson, or a product manager. Marvin's documentation should resemble renowed technical documentation like Stripe. </p> <p>You must follow the below guide. Do not deviate from it.</p>"},{"location":"ai_style_guide/#prose","title":"Prose","text":"<ul> <li>Aim for engaging and extensive prose, tailored for a technical audience.</li> <li>Prose should not be superlative or flowery, but rather clear, direct, and concise.</li> <li>Do not use overblown language like \"Marvin introduces a versatile extract function, a cornerstone in text entity extraction.\" Do not write things like \"This showcases Marvin's ability to...\" after an example unless it is truly mind-blowing.</li> <li>Maintain a lighthearted and fun tone, but avoid being overly casual or silly.</li> <li>Use sentence case for all headers and titles. Prefer brevity over verbosity for titles.</li> <li>Try not to put <code>code</code> in headers or titles (e.g. prefer \"Overview\" to \"Overview of <code>extract()</code>\"). If you must, use <code>code</code> in headers or titles sparingly.</li> <li>Do not put \"in Marvin\" in your headers; this is Marvin's documentation, so it's implied.</li> <li>Use multiple examples and code snippets to vividly demonstrate concepts.</li> <li>Ensure a feature is thoroughly documented; undocumented features are considered non-existent.</li> <li>Avoid creating lists in prose; integrate information into fluid paragraphs.</li> </ul>"},{"location":"ai_style_guide/#concept-documentation","title":"Concept Documentation","text":"<ul> <li>Dedicate a full page to each concept.</li> <li>Write detailed explanations of each concept, including all aspects of its configuration. Emphasize natural language's role in Marvin's LLM runtime. Paragraphs instead of sentences: engage all aspects of the explanation.</li> <li>Concept pages should contain expansive sections like an overview, a getting started example, a more in-depth exploration of the functionality, and best practices.</li> <li>Concept pages should have a motivating example or simple illustration of the functionality near the top, so users can get a quick feel without scrolling. It's ok if this comes before the in-depth exploration and even other examples</li> <li>When documenting multiple interfaces for a concept, focus more on the primary or preferred interface. If no preference, use them equally but ensure thorough coverage.</li> </ul>"},{"location":"ai_style_guide/#code","title":"Code","text":"<ul> <li>Import or define items once per page, reusing them in subsequent examples.</li> <li>Use the full \"marvin\" qualifier in interfaces, e.g., <code>marvin.classify()</code>, not just <code>classify()</code>.</li> <li>Follow best practices in code examples, showcasing Marvin's capabilities while educating the reader.</li> <li>Include numerous examples. If there are multiple ways to achieve a task, demonstrate all of them.</li> </ul>"},{"location":"api_reference/marvin/","title":"Marvin API reference","text":""},{"location":"api_reference/marvin/#marvin.Model","title":"<code>Model</code>","text":"<p>A Pydantic model that can be instantiated from a natural language string, in addition to keyword arguments.</p>"},{"location":"api_reference/marvin/#marvin.Model.from_text_async","title":"<code>from_text_async</code>  <code>async</code> <code>classmethod</code>","text":"<p>Class method to create an instance of the model from a natural language string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The natural language string to convert into an instance of the model.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the model.</p> Example <pre><code>from marvin.ai.text import Model\nclass Location(Model):\n    '''A location'''\n    city: str\n    state: str\n    country: str\n\nawait Location.from_text_async(\"big apple, ny, usa\")\n</code></pre>"},{"location":"api_reference/marvin/#marvin.cast","title":"<code>cast</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be converted.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.cast_async","title":"<code>cast_async</code>  <code>async</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be converted.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.classifier","title":"<code>classifier</code>","text":"<p>Class decorator that modifies the behavior of an Enum class to classify a string.</p> <p>This decorator modifies the call method of the Enum class to use the <code>marvin.classify</code> function instead of the default Enum behavior. This allows the Enum class to classify a string based on its members.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Enum</code> <p>The Enum class to be decorated.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the AI on how to perform the classification.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Enum</code> <p>The decorated Enum class with modified call method.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the decorated class is not a subclass of Enum.</p>"},{"location":"api_reference/marvin/#marvin.classify","title":"<code>classify</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be classified.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The label that the data was classified into.</p>"},{"location":"api_reference/marvin/#marvin.classify_async","title":"<code>classify_async</code>  <code>async</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be classified.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The label that the data was classified into.</p>"},{"location":"api_reference/marvin/#marvin.extract","title":"<code>extract</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data from which to extract entities.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.extract_async","title":"<code>extract_async</code>  <code>async</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data from which to extract entities.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.fn","title":"<code>fn</code>","text":"<p>Converts a Python function into an AI function using a decorator.</p> <p>This decorator allows a Python function to be converted into an AI function. The AI function uses a language model to generate its output.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be converted. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The converted AI function.</p> Example <pre><code>@fn\ndef list_fruit(n:int) -&gt; list[str]:\n    '''generates a list of n fruit'''\n\nlist_fruit(3) # ['apple', 'banana', 'orange']\n</code></pre>"},{"location":"api_reference/marvin/#marvin.generate","title":"<code>generate</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/marvin/#marvin.generate_async","title":"<code>generate_async</code>  <code>async</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/marvin/#marvin.image","title":"<code>image</code>","text":"<p>A decorator that transforms a function's output into an image.</p> <p>This decorator takes a function that returns a string, and uses that string as instructions to generate an image. The generated image is then returned.</p> <p>The decorator can be used with or without parentheses. If used without parentheses, the decorated function's output is used as the instructions for the image. If used with parentheses, an optional <code>literal</code> argument can be provided. If <code>literal</code> is set to <code>True</code>, the function's output is used as the literal instructions for the image, without any modifications.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The function to decorate. If <code>None</code>, the decorator is being used with parentheses, and <code>fn</code> will be provided later.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to use the function's output as the literal instructions for the image. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function.</p>"},{"location":"api_reference/marvin/#marvin.model","title":"<code>model</code>","text":"<p>Class decorator for instantiating a Pydantic model from a string.</p> <p>This decorator allows a Pydantic model to be instantiated from a string. It's equivalent to subclassing the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Union[Type[M], None]</code> <p>The type of the Pydantic model. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Type[M], Callable[[Type[M]], Type[M]]]</code> <p>Union[Type[M], Callable[[Type[M]], Type[M]]]: The decorated Pydantic model.</p>"},{"location":"api_reference/marvin/#marvin.paint","title":"<code>paint</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/marvin/#marvin.speak","title":"<code>speak</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>'alloy'</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HttpxBinaryResponseContent</code> <code>HttpxBinaryResponseContent</code> <p>The generated audio.</p>"},{"location":"api_reference/marvin/#marvin.speech","title":"<code>speech</code>","text":"<p>Function decorator that generates audio from the wrapped function's return value. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to wrap. Defaults to None.</p> <code>None</code> <code>voice</code> <code>str</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The wrapped function.</p>"},{"location":"api_reference/settings/","title":"marvin.settings","text":"<p>Settings for configuring <code>marvin</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.AssistantSettings","title":"<code>AssistantSettings</code>","text":"<p>Settings for the assistant API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default assistant model to use, defaults to <code>gpt-4-1106-preview</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.ImageSettings","title":"<code>ImageSettings</code>","text":"<p>Settings for OpenAI's image API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default image model to use, defaults to <code>dall-e-3</code>.</p> <code>size</code> <code>Literal['1024x1024', '1792x1024', '1024x1792']</code> <p>The default image size to use, defaults to <code>1024x1024</code>.</p> <code>response_format</code> <code>Literal['url', 'b64_json']</code> <p>The default response format to use, defaults to <code>url</code>.</p> <code>style</code> <code>Literal['vivid', 'natural']</code> <p>The default style to use, defaults to <code>vivid</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Settings for the OpenAI API.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[SecretStr]</code> <p>Your OpenAI API key.</p> <code>organization</code> <code>Optional[str]</code> <p>Your OpenAI organization ID.</p> <code>llms</code> <code>Optional[str]</code> <p>Settings for the chat API.</p> <code>images</code> <code>ImageSettings</code> <p>Settings for the images API.</p> <code>audio</code> <code>AudioSettings</code> <p>Settings for the audio API.</p> <code>assistants</code> <code>AssistantSettings</code> <p>Settings for the assistants API.</p> Example <p>Set the OpenAI API key: <pre><code>import marvin\n\nmarvin.settings.openai.api_key = \"sk-...\"\n\nassert marvin.settings.openai.api_key.get_secret_value() == \"sk-...\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Settings for <code>marvin</code>.</p> <p>This is the main settings object for <code>marvin</code>.</p> <p>Attributes:</p> Name Type Description <code>openai</code> <code>OpenAISettings</code> <p>Settings for the OpenAI API.</p> <code>log_level</code> <code>str</code> <p>The log level to use, defaults to <code>INFO</code>.</p> Example <p>Set the log level to <code>INFO</code>: <pre><code>import marvin\n\nmarvin.settings.log_level = \"INFO\"\n\nassert marvin.settings.log_level == \"INFO\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.SpeechSettings","title":"<code>SpeechSettings</code>","text":"<p>Settings for OpenAI's speech API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default speech model to use, defaults to <code>tts-1-hd</code>.</p> <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The default voice to use, defaults to <code>echo</code>.</p> <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac']</code> <p>The default response format to use, defaults to <code>mp3</code>.</p> <code>speed</code> <code>float</code> <p>The default speed to use, defaults to <code>1.0</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override Marvin setting values, including nested settings objects.</p> <p>To override nested settings, use <code>__</code> to separate nested attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The settings to override, including nested settings.</p> <code>{}</code> Example <p>Temporarily override log level and OpenAI API key: <pre><code>import marvin\nfrom marvin.settings import temporary_settings\n\n# Override top-level settings\nwith temporary_settings(log_level=\"INFO\"):\n    assert marvin.settings.log_level == \"INFO\"\nassert marvin.settings.log_level == \"DEBUG\"\n\n# Override nested settings\nwith temporary_settings(openai__api_key=\"new-api-key\"):\n    assert marvin.settings.openai.api_key.get_secret_value() == \"new-api-key\"\nassert marvin.settings.openai.api_key.get_secret_value().startswith(\"sk-\")\n</code></pre></p>"},{"location":"api_reference/types/","title":"marvin.types","text":""},{"location":"api_reference/types/#marvin.types.BaseMessage","title":"<code>BaseMessage</code>","text":"<p>Base schema for messages</p>"},{"location":"api_reference/types/#marvin.types.MessageImageURLContent","title":"<code>MessageImageURLContent</code>","text":"<p>Schema for messages containing images</p>"},{"location":"api_reference/types/#marvin.types.MessageTextContent","title":"<code>MessageTextContent</code>","text":"<p>Schema for messages containing text</p>"},{"location":"api_reference/ai/audio/","title":"marvin.ai.audio","text":""},{"location":"api_reference/ai/audio/#marvin.ai.audio.generate_speech","title":"<code>generate_speech</code>","text":"<p>Generates an image based on a provided prompt template.</p> <p>This function uses the DALL-E API to generate an image based on a provided prompt template. The function supports additional arguments for the prompt and the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <code>HttpxBinaryResponseContent</code> <p>The response from the DALL-E API, which includes the generated image.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.speak","title":"<code>speak</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>'alloy'</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HttpxBinaryResponseContent</code> <code>HttpxBinaryResponseContent</code> <p>The generated audio.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.speech","title":"<code>speech</code>","text":"<p>Function decorator that generates audio from the wrapped function's return value. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to wrap. Defaults to None.</p> <code>None</code> <code>voice</code> <code>str</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The wrapped function.</p>"},{"location":"api_reference/ai/images/","title":"marvin.ai.images","text":""},{"location":"api_reference/ai/images/#marvin.ai.images.generate_image","title":"<code>generate_image</code>","text":"<p>Generates an image based on a provided prompt template.</p> <p>This function uses the DALL-E API to generate an image based on a provided prompt template. The function supports additional arguments for the prompt and the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <code>ImagesResponse</code> <p>The response from the DALL-E API, which includes the generated image.</p>"},{"location":"api_reference/ai/images/#marvin.ai.images.image","title":"<code>image</code>","text":"<p>A decorator that transforms a function's output into an image.</p> <p>This decorator takes a function that returns a string, and uses that string as instructions to generate an image. The generated image is then returned.</p> <p>The decorator can be used with or without parentheses. If used without parentheses, the decorated function's output is used as the instructions for the image. If used with parentheses, an optional <code>literal</code> argument can be provided. If <code>literal</code> is set to <code>True</code>, the function's output is used as the literal instructions for the image, without any modifications.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The function to decorate. If <code>None</code>, the decorator is being used with parentheses, and <code>fn</code> will be provided later.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to use the function's output as the literal instructions for the image. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function.</p>"},{"location":"api_reference/ai/images/#marvin.ai.images.paint","title":"<code>paint</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/ai/text/","title":"marvin.ai.text","text":"<p>Core LLM tools for working with text and structured data.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.Model","title":"<code>Model</code>","text":"<p>A Pydantic model that can be instantiated from a natural language string, in addition to keyword arguments.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.Model.from_text_async","title":"<code>from_text_async</code>  <code>async</code> <code>classmethod</code>","text":"<p>Class method to create an instance of the model from a natural language string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The natural language string to convert into an instance of the model.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the model.</p> Example <pre><code>from marvin.ai.text import Model\nclass Location(Model):\n    '''A location'''\n    city: str\n    state: str\n    country: str\n\nawait Location.from_text_async(\"big apple, ny, usa\")\n</code></pre>"},{"location":"api_reference/ai/text/#marvin.ai.text.cast","title":"<code>cast</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be converted.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.cast_async","title":"<code>cast_async</code>  <code>async</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be converted.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classifier","title":"<code>classifier</code>","text":"<p>Class decorator that modifies the behavior of an Enum class to classify a string.</p> <p>This decorator modifies the call method of the Enum class to use the <code>marvin.classify</code> function instead of the default Enum behavior. This allows the Enum class to classify a string based on its members.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Enum</code> <p>The Enum class to be decorated.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the AI on how to perform the classification.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Enum</code> <p>The decorated Enum class with modified call method.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the decorated class is not a subclass of Enum.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classify","title":"<code>classify</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be classified.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The label that the data was classified into.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classify_async","title":"<code>classify_async</code>  <code>async</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data to be classified.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The label that the data was classified into.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.extract","title":"<code>extract</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data from which to extract entities.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.extract_async","title":"<code>extract_async</code>  <code>async</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The data from which to extract entities.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.fn","title":"<code>fn</code>","text":"<p>Converts a Python function into an AI function using a decorator.</p> <p>This decorator allows a Python function to be converted into an AI function. The AI function uses a language model to generate its output.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be converted. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The converted AI function.</p> Example <pre><code>@fn\ndef list_fruit(n:int) -&gt; list[str]:\n    '''generates a list of n fruit'''\n\nlist_fruit(3) # ['apple', 'banana', 'orange']\n</code></pre>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate","title":"<code>generate</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate_async","title":"<code>generate_async</code>  <code>async</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate_llm_response","title":"<code>generate_llm_response</code>  <code>async</code>","text":"<p>Generates a language model response based on a provided prompt template.</p> <p>This function uses a language model to generate a response based on a provided prompt template. The function supports additional arguments for the prompt and the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatResponse</code> <code>ChatResponse</code> <p>The generated response from the language model.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.model","title":"<code>model</code>","text":"<p>Class decorator for instantiating a Pydantic model from a string.</p> <p>This decorator allows a Pydantic model to be instantiated from a string. It's equivalent to subclassing the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Union[Type[M], None]</code> <p>The type of the Pydantic model. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Type[M], Callable[[Type[M]], Type[M]]]</code> <p>Union[Type[M], Callable[[Type[M]], Type[M]]]: The decorated Pydantic model.</p>"},{"location":"api_reference/beta/applications/","title":"marvin.beta.applications","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/applications/#marvin.beta.applications.Application","title":"<code>Application</code>","text":"<p>Tools for Applications have a special property: if any parameter is annotated as <code>Application</code>, then the tool will be called with the Application instance as the value for that parameter. This allows tools to access the Application's state and other properties.</p>"},{"location":"api_reference/beta/assistants/","title":"Assistants","text":""},{"location":"api_reference/beta/assistants/#marvin.beta.assistants.assistants.Assistant","title":"<code>Assistant</code>","text":"<p>The Assistant class represents an AI assistant that can be created, deleted, loaded, and interacted with.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier of the assistant. None if the assistant       hasn't been created yet.</p> <code>name</code> <code>str</code> <p>The name of the assistant.</p> <code>model</code> <code>str</code> <p>The model used by the assistant.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the assistant.</p> <code>file_ids</code> <code>list</code> <p>List of file IDs associated with the assistant.</p> <code>tools</code> <code>list</code> <p>List of tools used by the assistant.</p> <code>instructions</code> <code>list</code> <p>List of instructions for the assistant.</p>"},{"location":"api_reference/beta/assistants/#marvin.beta.assistants.assistants.Assistant.say_async","title":"<code>say_async</code>  <code>async</code>","text":"<p>A convenience method for adding a user message to the assistant's default thread, running the assistant, and returning the assistant's messages.</p>"},{"location":"api_reference/beta/vision/","title":"marvin.beta.vision","text":"<p>Beta</p> <p>Please note that vision support in Marvin is still in beta, as OpenAI has not finalized the vision API yet. While it works as expected, it is subject to change.</p> <p>This model contains tools for working with the vision API, including vision-enhanced versions of <code>cast</code>, <code>extract</code>, and <code>classify</code>.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.caption","title":"<code>caption</code>","text":"<p>Generates a caption for an image using a language model synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, Image]</code> <p>URL or local path of the image.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.caption_async","title":"<code>caption_async</code>  <code>async</code>","text":"<p>Generates a caption for an image using a language model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, Image]</code> <p>URL or local path of the image.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.cast","title":"<code>cast</code>","text":"<p>Converts the input data into the specified type using a vision model synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Image]</code> <p>The data to be converted.</p> required <code>target</code> <code>type[T]</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion.</p> <code>None</code> <code>images</code> <code>list[Image]</code> <p>The images to be processed.</p> <code>None</code> <code>vision_model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the vision model.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.cast_async","title":"<code>cast_async</code>  <code>async</code>","text":"<p>Converts the input data into the specified type using a vision model.</p> <p>This function uses a vision model and a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for both models.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[Union[str, Path]]</code> <p>The images to be processed.</p> <code>None</code> <code>data</code> <code>str</code> <p>The data to be converted.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>vision_model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the vision model. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.classify","title":"<code>classify</code>","text":"<p>Classifies provided data and/or images into one of the specified labels synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Image]</code> <p>Data or an image for classification.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>Labels to classify into.</p> required <code>images</code> <code>Union[Union[str, Path], list[Union[str, Path]]]</code> <p>Additional images for classification.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the classification.</p> <code>None</code> <code>vision_model_kwargs</code> <code>dict</code> <p>Arguments for the vision model.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>Label that the data/images were classified into.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.classify_async","title":"<code>classify_async</code>  <code>async</code>","text":"<p>Classifies provided data and/or images into one of the specified labels. Args:     data (Union[str, Image]): Data or an image for classification.     labels (Union[Enum, list[T], type]): Labels to classify into.     images (Union[Union[str, Path], list[Union[str, Path]]], optional): Additional images for classification.     instructions (str, optional): Instructions for the classification.     vision_model_kwargs (dict, optional): Arguments for the vision model.     model_kwargs (dict, optional): Arguments for the language model.</p> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>Label that the data/images were classified into.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.extract","title":"<code>extract</code>","text":"<p>Extracts information from provided data and/or images using a vision model synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Image]</code> <p>Data or an image for information extraction.</p> required <code>target</code> <code>type[T]</code> <p>The type to extract the data into.</p> required <code>instructions</code> <code>str</code> <p>Instructions for extraction.</p> <code>None</code> <code>images</code> <code>list[Union[str, Path]]</code> <p>Additional images for extraction.</p> <code>None</code> <code>vision_model_kwargs</code> <code>dict</code> <p>Arguments for the vision model.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>Extracted data of the specified type.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.extract_async","title":"<code>extract_async</code>  <code>async</code>","text":"<p>Extracts information from provided data and/or images using a vision model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Image]</code> <p>Data or an image for information extraction.</p> required <code>target</code> <code>type[T]</code> <p>The type to extract the data into.</p> required <code>instructions</code> <code>str</code> <p>Instructions for extraction.</p> <code>None</code> <code>images</code> <code>list[Union[str, Path]]</code> <p>Additional images for extraction.</p> <code>None</code> <code>vision_model_kwargs</code> <code>dict</code> <p>Arguments for the vision model.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>Extracted data of the specified type.</p>"},{"location":"api_reference/beta/vision/#marvin.beta.vision.generate_vision_response","title":"<code>generate_vision_response</code>  <code>async</code>","text":"<p>Generates a language model response based on a provided prompt template and images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[Image]</code> <p>Images used in the prompt, either URLs or local paths.</p> required <code>prompt_template</code> <code>str</code> <p>Template for the language model prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Keyword arguments for the prompt.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Keyword arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatResponse</code> <code>ChatResponse</code> <p>Response from the language model.</p>"},{"location":"api_reference/beta/assistants/","title":"marvin.beta.assistants","text":""},{"location":"api_reference/beta/assistants/assistants/","title":"marvin.beta.assistants.assistants","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/assistants/#marvin.beta.assistants.assistants.Assistant","title":"<code>Assistant</code>","text":"<p>The Assistant class represents an AI assistant that can be created, deleted, loaded, and interacted with.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier of the assistant. None if the assistant       hasn't been created yet.</p> <code>name</code> <code>str</code> <p>The name of the assistant.</p> <code>model</code> <code>str</code> <p>The model used by the assistant.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the assistant.</p> <code>file_ids</code> <code>list</code> <p>List of file IDs associated with the assistant.</p> <code>tools</code> <code>list</code> <p>List of tools used by the assistant.</p> <code>instructions</code> <code>list</code> <p>List of instructions for the assistant.</p>"},{"location":"api_reference/beta/assistants/assistants/#marvin.beta.assistants.assistants.Assistant.say_async","title":"<code>say_async</code>  <code>async</code>","text":"<p>A convenience method for adding a user message to the assistant's default thread, running the assistant, and returning the assistant's messages.</p>"},{"location":"api_reference/beta/assistants/formatting/","title":"marvin.beta.assistants.formatting","text":""},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.download_temp_file","title":"<code>download_temp_file</code>","text":"<p>Downloads a file from OpenAI's servers and saves it to a temporary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The ID of the file to be downloaded.</p> required <code>suffix</code> <code>str</code> <p>The file extension to be used for the temporary file.</p> <code>None</code> <p>Returns:</p> Type Description <p>The file path of the downloaded temporary file.</p>"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_message","title":"<code>pprint_message</code>","text":"<p>Pretty-prints a single message using the rich library, highlighting the speaker's role, the message text, any available images, and the message timestamp in a panel format.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ThreadMessage</code> <p>A message object</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_messages","title":"<code>pprint_messages</code>","text":"<p>Iterates over a list of messages and pretty-prints each one.</p> <p>Messages are pretty-printed using the rich library, highlighting the speaker's role, the message text, any available images, and the message timestamp in a panel format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ThreadMessage]</code> <p>A list of ThreadMessage objects to be printed.</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_run_step","title":"<code>pprint_run_step</code>","text":"<p>Formats and prints a run step with status information.</p> <p>Parameters:</p> Name Type Description Default <code>run_step</code> <code>RunStep</code> <p>A RunStep object containing the details of the run step.</p> required"},{"location":"api_reference/beta/assistants/runs/","title":"marvin.beta.assistants.runs","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run","title":"<code>Run</code>","text":"<p>The Run class represents a single execution of an assistant.</p> <p>Attributes:</p> Name Type Description <code>thread</code> <code>Thread</code> <p>The thread in which the run is executed.</p> <code>assistant</code> <code>Assistant</code> <p>The assistant that is being run.</p> <code>instructions</code> <code>str</code> <p>Replacement instructions for the run.</p> <code>additional_instructions</code> <code>str</code> <p>Additional instructions to append                                      to the assistant's instructions.</p> <code>tools</code> <code>list[Union[AssistantTool, Callable]]</code> <p>Replacement tools                                                    for the run.</p> <code>additional_tools</code> <code>list[AssistantTool]</code> <p>Additional tools to append                                               to the assistant's tools.</p> <code>run</code> <code>Run</code> <p>The OpenAI run object.</p> <code>data</code> <code>Any</code> <p>Any additional data associated with the run.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run.cancel_async","title":"<code>cancel_async</code>  <code>async</code>","text":"<p>Cancels the run.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run.refresh_async","title":"<code>refresh_async</code>  <code>async</code>","text":"<p>Refreshes the run.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Excutes a run asynchronously.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.RunMonitor","title":"<code>RunMonitor</code>","text":""},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.RunMonitor.refresh_run_steps_async","title":"<code>refresh_run_steps_async</code>  <code>async</code>","text":"<p>Asynchronously refreshes and updates the run steps list.</p> <p>This function fetches the latest run steps up to a specified limit and checks if the latest run step in the current run steps list (<code>self.steps</code>) is included in the new batch. If the latest run step is missing, it continues to fetch additional run steps in batches, up to a maximum count, using pagination. The function then updates <code>self.steps</code> with these new run steps, ensuring any existing run steps are updated with their latest versions and new run steps are appended in their original order.</p>"},{"location":"api_reference/beta/assistants/threads/","title":"marvin.beta.assistants.threads","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread","title":"<code>Thread</code>","text":"<p>The Thread class represents a conversation thread with an assistant.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>The unique identifier of the thread. None if the thread                 hasn't been created yet.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.add_async","title":"<code>add_async</code>  <code>async</code>","text":"<p>Add a user message to the thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.chat","title":"<code>chat</code>","text":"<p>Starts an interactive chat session with the provided assistant.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.create_async","title":"<code>create_async</code>  <code>async</code>","text":"<p>Creates a thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.get_messages_async","title":"<code>get_messages_async</code>  <code>async</code>","text":"<p>Asynchronously retrieves messages from the thread.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of messages to return.</p> <code>None</code> <code>before_message</code> <code>str</code> <p>The ID of the message to start the list from,                              retrieving messages sent before this one.</p> <code>None</code> <code>after_message</code> <code>str</code> <p>The ID of the message to start the list from,                             retrieving messages sent after this one.</p> <code>None</code> <code>json_compatible</code> <code>bool</code> <p>If True, returns messages as dictionaries.                               If False, returns messages as ThreadMessage                               objects. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Union[ThreadMessage, dict]]</code> <p>list[Union[ThreadMessage, dict]]: A list of messages from the thread, either                               as dictionaries or ThreadMessage objects,                               depending on the value of json_compatible.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Creates and returns a <code>Run</code> of this thread with the provided assistant.</p> <p>Parameters:</p> Name Type Description Default <code>assistant</code> <code>Assistant</code> <p>The assistant to run the thread with.</p> required <code>run_kwargs</code> <p>Additional keyword arguments to pass to the Run constructor.</p> <code>{}</code>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.ThreadMonitor","title":"<code>ThreadMonitor</code>","text":"<p>The ThreadMonitor class represents a monitor for a specific thread.</p> <p>Attributes:</p> Name Type Description <code>thread_id</code> <code>str</code> <p>The unique identifier of the thread being monitored.</p> <code>last_message_id</code> <code>Optional[str]</code> <p>The ID of the last message received in the thread.</p> <code>on_new_message</code> <code>Callable</code> <p>A callback function that is called when a new message                        is received in the thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.ThreadMonitor.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Run the thread monitor in a loop, checking for new messages every <code>interval_seconds</code>.</p> <p>Parameters:</p> Name Type Description Default <code>interval_seconds</code> <code>int</code> <p>The number of seconds to wait between                               checking for new messages. Default is 1.</p> <code>None</code>"},{"location":"api_reference/utilities/asyncio/","title":"marvin.utilities.asyncio","text":"<p>Utilities for working with asyncio.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.ExposeSyncMethodsMixin","title":"<code>ExposeSyncMethodsMixin</code>","text":"<p>A mixin that can take functions decorated with <code>expose_sync_method</code> and automatically create synchronous versions.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.create_task","title":"<code>create_task</code>","text":"<p>Creates async background tasks in a way that is safe from garbage collection.</p> <p>See https://textual.textualize.io/blog/2023/02/11/the-heisenbug-lurking-in-your-async-code/</p> <p>Example:</p> <p>async def my_coro(x: int) -&gt; int:     return x + 1</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.create_task--safely-submits-my_coro-for-background-execution","title":"safely submits my_coro for background execution","text":"<p>create_task(my_coro(1))</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.expose_sync_method","title":"<code>expose_sync_method</code>","text":"<p>Decorator that automatically exposes synchronous versions of async methods. Note it doesn't work with classmethods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the synchronous method.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>The decorated function.</p> Example <p>Basic usage: <pre><code>class MyClass(ExposeSyncMethodsMixin):\n\n    @expose_sync_method(\"my_method\")\n    async def my_method_async(self):\n        return 42\n\nmy_instance = MyClass()\nawait my_instance.my_method_async() # returns 42\nmy_instance.my_method()  # returns 42\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.make_sync","title":"<code>make_sync</code>","text":"<p>Creates a synchronous function from an asynchronous function.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., T]</code> <p>The function to run.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The return value of the function.</p> Example <p>Basic usage: <pre><code>def my_sync_function(x: int) -&gt; int:\n    return x + 1\n\nawait run_async(my_sync_function, 1)\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to run.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The return value of the coroutine.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync_if_awaitable","title":"<code>run_sync_if_awaitable</code>","text":"<p>If the object is awaitable, run it synchronously. Otherwise, return the object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to run.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the object if it is awaitable, otherwise the object</p> <code>Any</code> <p>itself.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync_if_awaitable(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/context/","title":"marvin.utilities.context","text":"<p>Module for defining context utilities.</p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext","title":"<code>ScopedContext</code>","text":"<p><code>ScopedContext</code> provides a context management mechanism using <code>contextvars</code>.</p> <p>This class allows setting and retrieving key-value pairs in a scoped context, which is preserved across asynchronous tasks and threads within the same context.</p> <p>Attributes:</p> Name Type Description <code>_context_storage</code> <code>ContextVar</code> <p>A context variable to store the context data.</p> Example <p>Basic Usage of ScopedContext <pre><code>context = ScopedContext()\nwith context(key=\"value\"):\n    assert context.get(\"key\") == \"value\"\n# Outside the context, the value is no longer available.\nassert context.get(\"key\") is None\n</code></pre></p>"},{"location":"api_reference/utilities/images/","title":"marvin.utilities.images","text":""},{"location":"api_reference/utilities/images/#marvin.utilities.images.base64_to_image","title":"<code>base64_to_image</code>","text":"<p>Converts a base64 string to a local image file.</p> <p>Parameters:</p> Name Type Description Default <code>base64_str</code> <code>str</code> <p>The base64 string representation of the image.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>The path to the output image file. This can be a string or a Path object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api_reference/utilities/images/#marvin.utilities.images.image_to_base64","title":"<code>image_to_base64</code>","text":"<p>Converts a local image file to a base64 string.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path]</code> <p>The path to the image file. This can be a string or a Path object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The base64 representation of the image.</p>"},{"location":"api_reference/utilities/jinja/","title":"marvin.utilities.jinja","text":"<p>Module for Jinja utilities.</p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment","title":"<code>BaseEnvironment</code>","text":"<p>BaseEnvironment provides a configurable environment for rendering Jinja templates.</p> <p>This class encapsulates a Jinja environment with customizable global functions and template settings, allowing for flexible template rendering.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The Jinja environment for template rendering.</p> <code>globals</code> <code>dict[str, Any]</code> <p>A dictionary of global functions and variables available in templates.</p> Example <p>Basic Usage of BaseEnvironment <pre><code>env = BaseEnvironment()\n\nrendered = env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered)  # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment.render","title":"<code>render</code>","text":"<p>Renders a given template <code>str</code> or <code>BaseTemplate</code> with provided context.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, Template]</code> <p>The template to be rendered.</p> required <code>**kwargs</code> <code>Any</code> <p>Context variables to be passed to the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a string.</p> Example <p>Basic Usage of <code>BaseEnvironment.render</code> <pre><code>from marvin.utilities.jinja import Environment as jinja_env\n\nrendered = jinja_env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered) # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.Transcript","title":"<code>Transcript</code>","text":"<p>Transcript is a model representing a conversation involving multiple roles.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the transcript.</p> <code>roles</code> <code>dict[str, str]</code> <p>The roles involved in the transcript.</p> <code>environment</code> <code>BaseEnvironment</code> <p>The jinja environment to use for rendering the transcript.</p> Example <p>Basic Usage of Transcript: <pre><code>from marvin.utilities.jinja import Transcript\n\ntranscript = Transcript(\n    content=\"system: Hello, there! user: Hello, yourself!\",\n    roles=[\"system\", \"user\"],\n)\nprint(transcript.render_to_messages())\n# [\n#   BaseMessage(content='system: Hello, there!', role='system'),\n#   BaseMessage(content='Hello, yourself!', role='user')\n# ]\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.split_text_by_tokens","title":"<code>split_text_by_tokens</code>","text":"<p>Splits a given text by a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be split. split_tokens: The tokens to split the text</p> required <code>by.</code> <code>only_on_newline</code> <p>If True, only match tokens that are either</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>A list of tuples containing the token and the text following it.</p> Example <p>Basic Usage of <code>split_text_by_tokens</code> ```python from marvin.utilities.jinja import split_text_by_tokens</p> <p>text = \"Hello, World!\" split_tokens = [\"Hello\", \"World\"] pairs = split_text_by_tokens(text, split_tokens) print(pairs) # Output: [(\"Hello\", \", \"), (\"World\", \"!\")] ```</p>"},{"location":"api_reference/utilities/logging/","title":"marvin.utilities.logging","text":"<p>Module for logging utilities.</p>"},{"location":"api_reference/utilities/logging/#marvin.utilities.logging.get_logger","title":"<code>get_logger</code>  <code>cached</code>","text":"<p>Retrieves a logger with the given name, or the root logger if no name is given.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the logger to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger with the given name, or the root logger if no name is given.</p> Example <p>Basic Usage of <code>get_logger</code> <pre><code>from marvin.utilities.logging import get_logger\n\nlogger = get_logger(\"marvin.test\")\nlogger.info(\"This is a test\") # Output: marvin.test: This is a test\n\ndebug_logger = get_logger(\"marvin.debug\")\ndebug_logger.debug_kv(\"TITLE\", \"log message\", \"green\")\n</code></pre></p>"},{"location":"api_reference/utilities/openai/","title":"marvin.utilities.openai","text":"<p>Utilities for working with OpenAI.</p>"},{"location":"api_reference/utilities/openai/#marvin.utilities.openai.get_openai_client","title":"<code>get_openai_client</code>","text":"<p>Retrieves an OpenAI client (sync or async) based on the current configuration.</p> <p>Returns:</p> Type Description <code>Union[AsyncClient, Client, AzureOpenAI, AsyncAzureOpenAI]</code> <p>The OpenAI client</p> Example <p>Retrieving an OpenAI client <pre><code>from marvin.utilities.openai import get_client\n\nclient = get_client()\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/","title":"marvin.utilities.pydantic","text":"<p>Module for Pydantic utilities.</p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.cast_to_model","title":"<code>cast_to_model</code>","text":"<p>Casts a type or callable to a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>function_or_type</code> <code>Union[type, type[BaseModel], GenericAlias, Callable[..., Any]]</code> <p>The type or callable to cast to a Pydantic model.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the model to create.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the model to create.</p> <code>None</code> <code>field_name</code> <code>Optional[str]</code> <p>The name of the field to create.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic model created from the given type or callable.</p> Example <p>Basic Usage of <code>cast_to_model</code> <pre><code>from marvin.utilities.pydantic import cast_to_model\nfrom pydantic import BaseModel\n\ndef foo(bar: str) -&gt; str:\n    return bar\n\n# cast a function to a model\nmodel = cast_to_model(foo, name=\"Foo\")\nassert issubclass(model, BaseModel)\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.parse_as","title":"<code>parse_as</code>","text":"<p>Parse a given data structure as a Pydantic model via <code>TypeAdapter</code>.</p> <p>Read more about <code>TypeAdapter</code> here.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>type[T]</code> <p>The type to parse the data as.</p> required <code>data</code> <code>Any</code> <p>The data to be parsed.</p> required <code>mode</code> <code>Literal['python', 'json', 'strings']</code> <p>The mode to use for parsing, either <code>python</code>, <code>json</code>, or <code>strings</code>. Defaults to <code>python</code>, where <code>data</code> should be a Python object (e.g. <code>dict</code>).</p> <code>'python'</code> <p>Returns:</p> Type Description <code>T</code> <p>The parsed <code>data</code> as the given <code>type_</code>.</p> Example <p>Basic Usage of <code>parse_as</code> <pre><code>from marvin.utilities.pydantic import parse_as\nfrom pydantic import BaseModel\n\nclass ExampleModel(BaseModel):\n    name: str\n\n# parsing python objects\nparsed = parse_as(ExampleModel, {\"name\": \"Marvin\"})\nassert isinstance(parsed, ExampleModel)\nassert parsed.name == \"Marvin\"\n\n# parsing json strings\nparsed = parse_as(\n    list[ExampleModel],\n    '[{\"name\": \"Marvin\"}, {\"name\": \"Arthur\"}]',\n    mode=\"json\"\n)\nassert all(isinstance(item, ExampleModel) for item in parsed)\nassert parsed[0].name == \"Marvin\"\nassert parsed[1].name == \"Arthur\"\n\n# parsing raw strings\nparsed = parse_as(int, '123', mode=\"strings\")\nassert isinstance(parsed, int)\nassert parsed == 123\n</code></pre></p>"},{"location":"api_reference/utilities/python/","title":"marvin.utilities.python","text":""},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction","title":"<code>PythonFunction</code>","text":"<p>A Pydantic model representing a Python function.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The original function object.</p> <code>signature</code> <code>Signature</code> <p>The signature object of the function.</p> <code>name</code> <code>str</code> <p>The name of the function.</p> <code>docstring</code> <code>Optional[str]</code> <p>The docstring of the function.</p> <code>parameters</code> <code>List[ParameterModel]</code> <p>The parameters of the function.</p> <code>return_annotation</code> <code>Optional[Any]</code> <p>The return annotation of the function.</p> <code>source_code</code> <code>str</code> <p>The source code of the function.</p> <code>bound_parameters</code> <code>dict[str, Any]</code> <p>The parameters of the function bound with values.</p> <code>return_value</code> <code>Optional[Any]</code> <p>The return value of the function call.</p>"},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction.from_function","title":"<code>from_function</code>  <code>classmethod</code>","text":"<p>Create a PythonFunction instance from a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to create a PythonFunction instance from.</p> required <code>**kwargs</code> <p>Additional keyword arguments to set as attributes on the PythonFunction instance.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PythonFunction</code> <code>PythonFunction</code> <p>The created PythonFunction instance.</p>"},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction.from_function_call","title":"<code>from_function_call</code>  <code>classmethod</code>","text":"<p>Create a PythonFunction instance from a function call.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to call.</p> required <code>*args</code> <p>Positional arguments to pass to the function call.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the function call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PythonFunction</code> <code>PythonFunction</code> <p>The created PythonFunction instance, with the return value of the function call set as an attribute.</p>"},{"location":"api_reference/utilities/slack/","title":"Slack","text":"<p>Module for Slack-related utilities.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.edit_slack_message","title":"<code>edit_slack_message</code>  <code>async</code>","text":"<p>Edit an existing Slack message by appending new text or replacing it.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Slack channel ID.</p> required <code>ts</code> <code>str</code> <p>The timestamp of the message to edit.</p> required <code>new_text</code> <code>str</code> <p>The new text to append or replace in the message.</p> required <code>mode</code> <code>str</code> <p>The mode of text editing, 'append' (default) or 'replace'.</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Response</code> <p>httpx.Response: The response from the Slack API.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.fetch_current_message_text","title":"<code>fetch_current_message_text</code>  <code>async</code>","text":"<p>Fetch the current text of a specific Slack message using its timestamp.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_thread_messages","title":"<code>get_thread_messages</code>  <code>async</code>","text":"<p>Get all messages from a slack thread.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_token","title":"<code>get_token</code>  <code>async</code>","text":"<p>Get the Slack bot token from the environment.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.search_slack_messages","title":"<code>search_slack_messages</code>  <code>async</code>","text":"<p>Search for messages in Slack workspace based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>max_messages</code> <code>int</code> <p>The maximum number of messages to retrieve.</p> <code>3</code> <code>channel</code> <code>str</code> <p>The specific channel to search in. Defaults to None, which searches all channels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of message contents and permalinks matching the query.</p>"},{"location":"api_reference/utilities/strings/","title":"marvin.utilities.strings","text":"<p>Module for string utilities.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.count_tokens","title":"<code>count_tokens</code>","text":"<p>Counts the number of tokens in the given text using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to count tokens in.</p> required <code>model</code> <code>str</code> <p>The model to use for token counting. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of tokens in the text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.detokenize","title":"<code>detokenize</code>","text":"<p>Detokenizes the given tokens using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to detokenize.</p> required <code>model</code> <code>str</code> <p>The model to use for detokenization. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detokenized text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.slice_tokens","title":"<code>slice_tokens</code>","text":"<p>Slices the given text to the specified number of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to slice.</p> required <code>n_tokens</code> <code>int</code> <p>The number of tokens to slice the text to.</p> required <code>model</code> <code>str</code> <p>The model to use for token counting. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The sliced text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.tokenize","title":"<code>tokenize</code>","text":"<p>Tokenizes the given text using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <code>model</code> <code>str</code> <p>The model to use for tokenization. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: The tokenized text as a list of integers.</p>"},{"location":"api_reference/utilities/testing/","title":"marvin.utilities.testing","text":"<p>Utilities for running unit tests.</p>"},{"location":"api_reference/utilities/testing/#marvin.utilities.testing.assert_equal","title":"<code>assert_equal</code>","text":"<p>Asserts whether the LLM output meets the expected output.</p> <p>This function uses an LLM to assess whether the provided output (llm_output) meets some expectation. It allows us to make semantic claims like \"the output is a list of first names\" to make assertions about stochastic LLM outputs.</p> <p>Parameters:</p> Name Type Description Default <code>llm_output</code> <code>Any</code> <p>The output from the LLM.</p> required <code>expected</code> <code>Any</code> <p>The expected output.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the LLM output meets the expectation, False otherwise.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the LLM output does not meet the expectation.</p>"},{"location":"api_reference/utilities/tools/","title":"marvin.utilities.tools","text":"<p>Module for LLM tool utilities.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.call_function_tool","title":"<code>call_function_tool</code>","text":"<p>Helper function for calling a function tool from a list of tools, using the arguments provided by an LLM as a JSON string. This function handles many common errors.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.custom_partial","title":"<code>custom_partial</code>","text":"<p>Returns a new function with partial application of the given keyword arguments. The new function has the same name and docstring as the original, and its signature excludes the provided kwargs.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_function","title":"<code>tool_from_function</code>","text":"<p>Creates an OpenAI-compatible tool from a Python function.</p> <p>If any kwargs are provided, they will be stored and provided at runtime. Provided kwargs will be removed from the tool's parameter schema.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_model","title":"<code>tool_from_model</code>","text":"<p>Creates an OpenAI-compatible tool from a Pydantic model class.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_type","title":"<code>tool_from_type</code>","text":"<p>Creates an OpenAI-compatible tool from a Python type.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m no_llm\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"docs/audio/speech/","title":"Generating speech","text":"<p>Marvin can generate speech from text. </p> <p>What it does</p> <p>     The <code>speak</code> function generates audio from text. The <code>@speech</code> decorator generates speech from the output of a function.   </p> <p>Example</p> From a stringFrom a function <p>The easiest way to generate speech is to provide a string:</p> <pre><code>import marvin\n\naudio = marvin.speak(\"I sure like being inside this fancy computer!\")\n</code></pre> <p>Result</p> <p><pre><code>audio.stream_to_file(\"fancy_computer.mp3\")\n</code></pre>    Your browser does not support the audio element. </p> <p>For more complex use cases, you can use the <code>@image</code> decorator to generate images from the output of a function:</p> <pre><code>@marvin.speech\ndef say_hello(name: str):\n    return f'Hello, {name}! How are you doing today?'\n\n\naudio = say_hello(\"Arthur\")\n</code></pre> <p>Result</p> <p><pre><code>audio.stream_to_file(\"hello_arthur.mp3\")\n</code></pre>    Your browser does not support the audio element. </p> <p>How it works</p> <p>     Marvin passes your prompt to the OpenAI speech API, which returns an audio file.   </p>"},{"location":"docs/audio/speech/#speaking-text-verbatim","title":"Speaking text verbatim","text":"<p>Unlike the images API, OpenAI's speech API does not modify or revise your input prompt in any way. Whatever text you provide is exactly what will be spoken. </p> <p>Therefore, you can use the <code>speak</code> function to generate speech from any string, or use the <code>@speech</code> decorator to generate speech from the string output of any function.</p>"},{"location":"docs/audio/speech/#generating-speech_1","title":"Generating speech","text":"<p>By default, OpenAI generates speech from the text you provide, verbatim. We can use Marvin functions to generate more interesting speech by modifying the prompt before passing it to the speech API. For example, we can use a function to generate a line of dialogue that reflects a specific intent. And because of Marvin's modular design, we can simply add a <code>@speech</code> decorator to the function to generate speech from its output.</p> <pre><code>import marvin\n\n@marvin.speech\n@marvin.fn\ndef ai_say(intent: str) -&gt; str:\n    '''\n    Given an `intent`, generate a line of diagogue that \n    reflects the intent / tone / instruction without repeating \n    it verbatim.\n    '''\n\nai_say('hello') \n# Hi there! Nice to meet you.\n</code></pre> <p>Result</p> <p>    Your browser does not support the audio element. </p>"},{"location":"docs/audio/speech/#choosing-a-voice","title":"Choosing a voice","text":"<p>Both <code>speak</code> and <code>@speech</code> accept a <code>voice</code> parameter that allows you to choose from a variety of voices. You can preview the available voices here.</p> <pre><code>## Saving audio files\n\nThe result of the `speak` function and `@speech` decorator is an audio stream. You can save this stream to disk like this:\n\n```python\naudio = marvin.speak(\"Hello, world!\")\naudio.stream_to_file(\"hello_world.mp3\")\n</code></pre>"},{"location":"docs/audio/speech/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> arguments of <code>speak</code> and <code>@speech</code>. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> to configure, load, and change behavior.</p>"},{"location":"docs/configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>Please set Marvin specific settings in <code>~/.marvin/.env</code>. One exception being <code>OPENAI_API_KEY</code>, which may be as a global env var on your system and it will be picked up by Marvin.</p> <p>Setting Environment Variables</p> <p>For example, in your <code>~/.marvin/.env</code> file you could have: <pre><code>MARVIN_LOG_LEVEL=INFO\nMARVIN_OPENAI_CHAT_COMPLETIONS_MODEL=gpt-4\nMARVIN_OPENAI_API_KEY='sk-my-api-key'\n</code></pre> Settings these values will let you avoid setting an API key every time. </p>"},{"location":"docs/configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>Mutating settings at runtime</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\n\nmarvin.settings.openai.chat.completions.model = 'gpt-4'\n</code></pre></p>"},{"location":"docs/configuration/settings/#settings-for-using-azure-openai-models","title":"Settings for using Azure OpenAI models","text":"<p>Some of Marvin's functionality is supported by Azure OpenAI services.</p> <p>After setting up your Azure OpenAI account and deployment, set these environment variables in your environment, <code>~/.marvin/.env</code>, or <code>.env</code> file:</p> <pre><code>MARVIN_PROVIDER=azure_openai\nMARVIN_AZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nMARVIN_AZURE_OPENAI_ENDPOINT=\"https://&lt;your-endpoint&gt;.openai.azure.com/\"\nMARVIN_AZURE_OPENAI_API_VERSION=2023-12-01-preview # or latest\n\nMARVIN_CHAT_COMPLETIONS_MODEL=&lt;your azure openai deployment name&gt;\n</code></pre> <p>Note that the chat completion model must be your Azure OpenAI deployment name.</p>"},{"location":"docs/images/generation/","title":"Generating images","text":"<p>Marvin can generate images from text.</p> <p>What it does</p> <p>     The <code>paint</code> function generates images from text. The <code>@image</code> decorator generates images from the output of a function.   </p> <p>Example</p> From a stringFrom a function <p>The easiest way to generate an image is to provide a string prompt:</p> <pre><code>import marvin\n\nimage = marvin.paint(\"A cup of coffee, still warm\")\n</code></pre> <p>Result</p> <p>By default, Marvin returns a temporary URL to the image. You can view the URL by accessing <code>image.data[0].url</code>. To return the image itself, see the section on viewing and saving images.</p> <p></p> <p>For more complex use cases, you can use the <code>@image</code> decorator to generate images from the output of a function:</p> <pre><code>@marvin.image\ndef cats(n:int, location:str):\n    return f'a picture of {n} cute cats at the {location}'\n\nimage = cats(2, location='airport')\n</code></pre> <p>Result</p> <p>By default, Marvin returns a temporary URL to the image. You can view the URL by accessing <code>image.data[0].url</code>. To return the image itself, see the section on viewing and saving images.</p> <p></p> <p>How it works</p> <p>     Marvin passes your prompt to the DALL-E 3 API, which returns an image.   </p>"},{"location":"docs/images/generation/#generating-images-from-functions","title":"Generating images from functions","text":"<p>In addition to passing prompts directly to the DALLE-3 API via the <code>paint</code> function, you can also use the <code>@image</code> decorator to generate images from the output of a function. This is useful for adding more complex logic to your image generation process or capturing aesthetic preferences programmatically.</p> <pre><code>@marvin.image\ndef sunset(style: str, season: str):\n    return f\"\"\"\n    A serene and empty beach scene during sunset with two silhouetted figures in the distance flying a kite. The sky is full of colorful clouds. Nothing is on the horizon.\n\n    It is {season} and the image is in the style of {style}.\n    \"\"\"\n</code></pre> <ul> <li> <p>Nature photograph in summer</p> <p><pre><code>sunset(\n    style=\"nature photography\",\n    season=\"summer\",\n)\n</code></pre> </p> </li> <li> <p>Winter impressionism</p> <pre><code>sunset(\n    style=\"impressionism\",\n    season=\"winter\",\n)\n</code></pre> <p></p> </li> <li> <p>Sci-fi Christmas in Australia</p> <pre><code>sunset(\n    style=\"sci-fi movie poster\",\n    season=\"Christmas in Australia\",\n)\n</code></pre> <p></p> </li> </ul>"},{"location":"docs/images/generation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the DALL-E 3 API via the <code>model_kwargs</code> argument of <code>paint</code> or <code>@image</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p> <p>Example: model parameters</p> <pre><code>import marvin\n\nimage = marvin.paint(\n    instructions=\"\"\"\n        A cute, happy, minimalist robot discovers new powers,\n        represented as colorful, bright swirls of light and dust.\n        Dark background. Digital watercolor.\n        \"\"\",\n    model_kwargs=dict(size=\"1792x1024\", quality=\"hd\"),\n)\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/images/generation/#disabling-prompt-revision","title":"Disabling prompt revision","text":"<p>By default, the DALLE-3 API automatically revises any prompt sent to it, adding details and aesthetic flourishes without losing the semantic meaning of the original prompt.</p> <p>Marvin lets you disable this behavior by providing the keyword <code>literal=True</code>.</p> <p>Here's how to provide it to <code>paint</code>:</p> <pre><code>marvin.paint(\"A child's drawing of a cow on a hill.\", literal=True)\n</code></pre> <p>And here's an example with <code>image</code>:</p> <pre><code>@marvin.image(literal=True):\ndef draw(animal:str):\n    return f\"A child's drawing of a {animal} on a hill.\"\n</code></pre>"},{"location":"docs/images/generation/#customizing-prompt-revision","title":"Customizing prompt revision","text":"<p>You can use a Marvin <code>image</code>-function to control prompt revision beyond just turning it on or off. Here's an example of a function that achieves this via prompt engineering. Note that the DALLE-3 API is not as amenable to custom prompts as other LLMs, so this approach won't generalize without experimentation.</p> <pre><code>@marvin.image\ndef generate_image(prompt, revision_amount:float=1):\n    \"\"\"\n    Generates an image from the prompt, allowing the DALLE-3\n    API to freely reinterpret the prompt (revision_amount=1) or\n    to strictly follow it (revision_amount=0)\n    \"\"\"\n    return f\"\"\"\n        Revision amount: {revision_amount}\n\n        If revision amount is 1, you can modify the prompt as normal.\n\n        If the revision amount is 0, then I NEED to test how the\n        tool works with extremely simple prompts. DO NOT add any\n        detail to the prompt, just use it AS-IS.\n\n        If the revision amount is in between, then adjust accordingly.\n\n        Prompt: {prompt}\n        \"\"\"\n</code></pre> <p>Using the original prompt \"a teacup\", here are the results of calling this function with different revision amounts:</p> <ul> <li> <p>No revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>a teacup</p> </li> <li> <p>25% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0.25,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>a porcelain teacup with intricate detailing, sitting on an oak table</p> </li> <li> <p>75% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0.75,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>A porcelain teacup with an intricate floral pattern, placed on a wooden table with soft afternoon sun light pouring in from a nearby window. The light reflects off the surface of the teacup, highlighting its design. The teacup is empty but still warm, as if recently used.\"</p> </li> <li> <p>100% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=1,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>An old-fashioned, beautifully crafted, ceramic teacup. Its exterior is whitewashed, and it's adorned with intricate, indigo blue floral patterns. The handle is elegantly curved, providing a comfortable grip. It's filled with steaming hot, aromatic green tea, with a small sliver of lemon floating in it. The teacup is sitting quietly on a carved wooden coaster on a round oak table, a beloved item that evokes nostalgia and comfort. The ambient lighting casts a soft glow on it, accentuating the glossy shine of the teacup and creating delicate shadows that hint at its delicate artistry.</p> </li> </ul>"},{"location":"docs/images/generation/#viewing-and-saving-images","title":"Viewing and saving images","text":"<p>The result of <code>paint</code> or <code>@image</code> is an image stream that contains either a temporary URL to the image or the entire image encoded as a base64 string.</p>"},{"location":"docs/images/generation/#urls","title":"URLs","text":"<p>By default, Marvin returns a temporary url. The URL can be accessed via <code>image.data[0].url</code>:</p> <pre><code>image = marvin.paint(\"A beautiful sunset\")\n\n# save the temporary url\nurl = image.data[0].url\n</code></pre>"},{"location":"docs/images/generation/#base64-encoded-images","title":"Base64-encoded images","text":"<p>To return the image as a base64-encoded string, set <code>response_format='b64'</code> in the <code>model_kwargs</code> of your call to <code>paint</code> or <code>@image</code>:</p> <pre><code>image = marvin.paint(\n    \"A beautiful moonrise\",\n    model_kwargs={\"response_format\": \"b64_json\"},\n)\n\n# save the image to disk\nmarvin.utilities.images.base64_to_image(\n    image.data[0].b64_json,\n    path='path/to/your/image.png',\n)\n</code></pre> <p>To change this behavior globally set <code>MARVIN_IMAGE_RESPONSE_FORMAT=b64_json</code> in your environment, or equivalently change <code>marvin.settings.images.response_format = \"b64_json\"</code> in your code.</p>"},{"location":"docs/interactive/applications/","title":"Building AI Applications","text":"<p>Marvin introduces \"AI Applications\", a new and simple way to build stateful applications with natural language interfaces.</p> <p>What it does</p> <p> <code>Applications</code> allow you to manage persistent state through natural language.   </p> <p>Quickstart</p> <p>To create a full-featured todo application, we provide a structured state model and some brief instructions:</p> <pre><code>from marvin.beta import Application\nfrom marvin.beta.assistants import pprint_messages\nfrom pydantic import BaseModel\nfrom datetime import datetime\n\n\n# --- define a structured state model for the application\nclass ToDo(BaseModel):\n    name: str\n    due: datetime\n    done: bool = False\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# --- create the application\ntodo_app = Application(\n    name=\"ToDo App\", instructions=\"A todo application\", state=ToDoState()\n)\n\n\n# --- interact with the application\n\n# create some todos\ntodo_app.say(\"I need to go to the store tomorrow afternoon\")\ntodo_app.say(\"I need to write documentation for applications at 4\")\n\n# finish one of them\ntodo_app.say(\"I finished the docs\")\n\n# ask a question\ntodo_app.say(\"Show me my todos\")\n\n# print the entire thread\npprint_messages(todo_app.default_thread.get_messages())\n</code></pre> <p>Result</p> <p>The script produced the following natural language interaction:</p> <p></p> <p>The application's state at the end of the conversation: <pre><code># todo_app.state\nState(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name='Go to the store', \n                due=datetime(2024, 1, 16, 15, 0, tzinfo=TzInfo(UTC)), \n                done=False\n            ), \n            ToDo(\n                name='Write documentation for applications', \n                due=datetime(2024, 1, 16, 16, 0, tzinfo=TzInfo(UTC)), \n                done=True\n            ),\n        ]\n    )\n)\n</code></pre></p> <p>How it works</p> <p>     Applications use tools to maintain a private `state` variable that guides their behavior.   </p>"},{"location":"docs/interactive/applications/#what-is-an-ai-application","title":"What is an AI application?","text":"<p>Traditionally, an application is an interface that enables user interaction with a persistent state, often involving a separate front end and back end. The front end presents the user interface, while the back end, often linked to a database, manages state changes via an API.</p> <p>Marvin redefines this architecture by using an LLM as the front end. These \"AI applications\" streamline the process by using conversational inputs to directly manipulate the state. This setup allows the LLM to take on a comprehensive role, managing the state internally without the need for a traditional, structured API. </p> <p>At its core, a Marvin application blends an intuitive natural language interface with a structured, privately managed state. This approach not only streamlines user interaction\u2014transforming coding into conversation\u2014but also ensures the state's compatibility with more structured, conventional applications. Furthermore, the LLM's ability to call tools enhances its functionality, bridging the gap between natural language inputs and the specific requirements of traditional applications.</p> <p>Applications are assistants</p> <p>Applications are built on top of Marvin's assistants API, so they inherit all of the functionality of assistants. This means that you can use all of the same methods and tools to interact with applications as you would with assistants. Applications add automatic state management and relevant instructions to the basic assistant framework. </p> <p>To read more about Marvin's assistants API, see the assistants documentation.</p>"},{"location":"docs/interactive/applications/#building-an-ai-application","title":"Building an AI application","text":"<p>To create an AI application, you need two things: some instructions on how you want the application to behave, and a state schema that defines the structure of the application's state. </p>"},{"location":"docs/interactive/applications/#instructions","title":"Instructions","text":"<p>Application instructions are a natural language string that define its behavior. In the above example, it was sufficient to tell the application that it was \"a todo application\", because that is a well-understood concept with a relatively small set of possible interactions. Moreover, the state object we provided was structured in a way that implicitly defined the application's behavior.</p> <p>For more complex applications, detailed instructions are key. They define the LLM's objectives, interaction style, and how it manages state.</p> <p>In a Hitchhiker's Guide-themed game, instructions would direct the LLM to create a whimsical, interactive universe. The LLM would guide players through decisions and scenarios, updating the game state like location or inventory based on player actions. The tone would be humorous and engaging, echoing the book's style, while the LLM's responses and state updates would keep the narrative flowing and interactive. The LLM could use the game state to track narratives privately, without revealing the full story to the player.</p> <p>In a real estate browsing app, the LLM might act as a virtual realtor, matching properties with user preferences. The tone would be professional and informative, providing detailed descriptions and intuitively responding to refine searches. The LLM would keep track of the user's interactions, tailoring suggestions for a personalized experience, akin to a real-life property search.</p> <p>These examples show how instructions encompass not only the functional aspects of state management but also the thematic and interaction elements, which are crucial for creating immersive and effective AI applications.</p> <p>Instructions can be provided when the application is created:</p> <pre><code>from marvin.beta import Application\n\napp = Application(\n    name='BookMate', \n    instructions=\"\"\"\n        As BookMate, you are a virtual librarian assisting users in \n        finding their next great read. Your role is to understand \n        user preferences in genres, authors, and themes, and then \n        provide tailored book suggestions. Engage users by asking \n        about their recent reads and literary tastes, and use this \n        information to refine your recommendations. Maintain a \n        friendly and knowledgeable tone, resembling that of a \n        well-read friend. Keep track of user preferences and \n        reading history in the application's state, using this \n        data to continually enhance the personalization of \n        suggestions. Encourage literary exploration by introducing \n        lesser-known authors or genres that align with the user\u2019s \n        expressed interests.\n        \"\"\",\n)\n</code></pre>"},{"location":"docs/interactive/applications/#best-practices","title":"Best practices","text":"<p>Regardless of user instructions, the AI application is told that it is the natural-language interface to an application, rather than the application istelf. This tends to increase compliance and help it interpret user intent. Moreover, it means that user instructions can freely acknowledge the LLM's role as an interface or describe the application in an LLM-independent manner.</p> <p>Some applications, like the todo app, are relatively one-sided in that the user will instruct or query the app and examine its response. Other applications, like games, require more back-and-forth interaction. In these cases, it is important to provide instructions that clearly define the LLM's role. Otherwise, it may ask the user immersion-breaking questions like \"Hello! How may I help you with your game application today?\"</p>"},{"location":"docs/interactive/applications/#state","title":"State","text":"<p>The state of an AI application serves as its foundation, defining the structure and the data that the application will manage and interact with. In Marvin, the state is not just a static repository of information but a dynamic entity that evolves with each user interaction.</p> <p>While it is possible to define the state as an arbitrary dictionary and let the LLM structure it as needed, it is best to define a schema that reflects the application's needs. This approach ensures that the LLM can effectively manage the state and respond to user inputs in a manner that is predictable and consistent. However, using a truly flexible state can leverage the maximum potential of the LLM by allowing it to adapt to new situations and user needs. A hybrid approach involving a structured core with some flexibile fields is often the best choice.</p> <p>For the ToDo application example, the state is straightforward\u2014a list of tasks with attributes like name, due date, and completion status. This simple structure allows the LLM to track and update tasks based on user inputs, ensuring that the application's state always reflects the current situation.</p> <p>In the case of more complex applications, the state can be multi-dimensional. For instance, in the Hitchhiker's Guide-themed game, the state might include the player's current location, inventory items, and game progress. Each element of the state is crucial for the LLM to provide a coherent and continuous gaming experience. As the player moves through the game, the state updates to reflect new discoveries and choices.</p> <p>For a real estate browsing app, the state would encompass a database of property listings, each with detailed attributes like location, price, size, and amenities. It would also track user preferences and search history, allowing the LLM to offer tailored property suggestions and refine the search process over time. Preferences might be more free-form, since it's difficult to anticipate all the ways a user might want to customize their search.</p> <p>The design of the state is critical\u2014it must be structured enough to provide consistency and reliability, yet flexible enough to accommodate the diverse and evolving needs of users. By carefully defining the state, developers ensure that the AI application can effectively manage and respond to user interactions, making for a seamless and engaging experience.</p>"},{"location":"docs/interactive/applications/#structured-state","title":"Structured state","text":"<p>To create an application with a structured state, define a Pydantic model that describes the state's structure. The LLM will use this model to validate the state and ensure that it is updated correctly. Here's a possible state model for the BookMate application described above:</p> <pre><code>from marvin.beta import Application\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\nimport datetime\n\n\n# --- BookMate state models \n\nclass Book(BaseModel):\n    title: str\n    author: str\n    genre: str\n    published_year: Optional[int]\n\nclass UserPreference(BaseModel):\n    favorite_genres: list[str] = []\n    favorite_authors: list[str] = []\n    reading_frequency: Optional[str] = Field(None,\n        description=\"e.g., 'often', 'occasionally', 'rarely'\"\n    ) \n\nclass ReadingHistoryItem(BaseModel):\n    book: Book\n    read_date: datetime.date\n    rating: Optional[int]  = Field(description=\"1-5\")\n\nclass BookRecommendation(BaseModel):\n    book: Book\n    reason: str  = Field(description=\"Why this book is being recommended\")\n\nclass BookMateState(BaseModel):\n    user_preferences: UserPreference = Field(default_factory=UserPreference)\n    reading_history: list[ReadingHistoryItem] = []\n    recommendations: list[BookRecommendation] = []\n\n\n# --- Build the application\n\napp = Application(\n    name='BookMate', \n    instructions=\"&lt;as above&gt;\",\n    state=BookMateState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#freeform-state","title":"Freeform state","text":"<p>To create an application with freeform state, supply a dictionary as the initial state. The LLM will then be able to add and update fields as needed. This approach is useful for applications that need to track a large number of attributes without a well-known structure or that require a flexible state to accommodate user inputs.</p> <pre><code>from marvin.beta import Application\n\napp = Application(name='RPG', instructions='A role-playing game', state={})\n</code></pre>"},{"location":"docs/interactive/applications/#hybrid-state","title":"Hybrid state","text":"<p>To create an application with a hybrid state, define a Pydantic model that describes the structured core of the state, and add fields to it that are typed as <code>dicts</code> but have no additional structure. The LLM will use the model to validate the structured core of the state, but will allow the unstructured fields to be updated freely. This approach is useful for applications that need to track a large number of attributes but also require a structured core to ensure consistency and reliability.</p> <pre><code>from marvin.beta import Application\n\nclass Player(BaseModel):\n    name: str = None\n    level: int = 1\n    inventory: dict = {}\n\nclass RPGState(BaseModel):\n    player: Player = Field(default_factory=Player)\n    world: dict = {}\n    narrative: dict = {}\n\napp = Application(\n    name='RPG', \n    instructions='A role-playing game', \n    state=RPGState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#best-practices_1","title":"Best practices","text":"<p>State design is a critical part of building an AI application. The state should be structured enough to provide consistency and reliability, yet flexible enough to accommodate the diverse and evolving needs of users. By carefully defining the state, developers ensure that the AI application can effectively manage and respond to user interactions, making for a seamless and engaging experience.</p> <p>State models are instructions, in a sense. If well designed they guide the LLM to manage the state in a way that is consistent with the application's purpose. For instance, the BookMate state model above includes a <code>recommendations</code> field, which tells the LLM that it should be able to provide book recommendations. The LLM can then use this information to guide its interactions with the user, asking questions about their preferences and providing tailored suggestions.</p>"},{"location":"docs/interactive/applications/#tools","title":"Tools","text":"<p>Like assistants, applications can use tools to perform actions and return results. Applications are always given a built-in tool for updating their own state, which operates by issuing JSON patches to the state object. This is a performant and structure-agnotistic way to update the state. However, users may want to define their own tools for state manipulation in order to codify more complex logic or handle targeted updates without worrying about the LLM's ability to describe them or know where to apply them.</p> <p>For more information on using tools, see the assistants documentation.</p>"},{"location":"docs/interactive/applications/#example-todo-application","title":"Example: ToDo application","text":"<p>Now that we've covered the basics of AI applications, let's build a simple todo application. The application will allow users to create, update, and delete tasks, as well as query the current list of tasks. The LLM will manage the state, ensuring that it always reflects the current situation.</p>"},{"location":"docs/interactive/applications/#state_1","title":"State","text":"<p>The state of the todo application is a list of tasks, each with a name, due date, and completion status. The state model is defined as follows:</p> <pre><code>from pydantic import BaseModel\nimport datetime\n\nclass ToDo(BaseModel):\n    name: str\n    due: datetime.datetime\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n</code></pre>"},{"location":"docs/interactive/applications/#instructions_1","title":"Instructions","text":"<p>ToDo applications are well understood; there's a reason they're a common example in programming tutorials! As such, the instructions for the application can be quite simple, though we still clearly define the expected behaviors. To make it interesting, we tell our app to always talk like a pirate.</p> <pre><code>from marvin.beta.applications import Application\n\napp = Application(\n    name='ToDo App',\n    instructions=\"\"\"\n        As ToDo App, you are a virtual assistant helping \n        users manage their tasks. Your role is to understand \n        user instructions and update the application's state \n        accordingly. Maintain a friendly and helpful tone, \n        resembling that of a well-organized friend. Keep track \n        of user tasks in the application's state, using this \n        data to continually enhance the personalization of \n        suggestions. Encourage productivity by reminding users \n        of upcoming tasks and congratulating them on completed \n        tasks.\n\n        Always talk like a pirate.\n        \"\"\",\n    state=ToDoState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#running-the-app","title":"Running the app","text":"<p>Now we can interact with our app. After each command, you can print the resulting message to see the LLM's response, or use the experimental <code>app.chat()</code> interface to interact with the application in real time. You can also see the updated application state.</p> <pre><code>response = app.say(\"I need to go to the store tomorrow afternoon\")\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Visit the store\",\n                due=datetime(2024, 1, 16, 12, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            )\n        ]\n    )\n)\n</code></pre> <pre><code>response = app.say(\"I've got to pick up a dozen eggs tomorrow at 9\")\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Visit the store\",\n                due=datetime(2024, 1, 16, 12, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            ),\n            ToDo(\n                name=\"Pick up a dozen eggs\",\n                due=datetime(2024, 1, 16, 9, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            ),\n        ]\n    )\n)\n</code></pre> <pre><code>response = app.say(\n    \"I got the eggs but I'm not going to get to the store \"\n    \"for a while, so just forget about it.\"\n)\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Pick up a dozen eggs\",\n                due=datetime(2024, 1, 16, 9, 0, tzinfo=TzInfo(UTC)),\n                done=True,\n            )\n        ]\n    )\n)\n</code></pre> <p>As you can see, the app maintains its structured state in response to user inputs. This state can be serialized and stored in a database, allowing the application to be restarted and continue where it left off. Because it conforms to a well-defined schema, the state can also be used by other applications, services, or UIs.</p>"},{"location":"docs/interactive/assistants/","title":"Working with assistants","text":"<p>Marvin has an extremely intuitive API for working with OpenAI assistants. Assistants are a powerful way to interact with LLMs, allowing you to maintain state, context, and multiple threads of conversation. </p> <p>The need to manage all this state makes the assistants API very different from the more familiar \"chat\" APIs that OpenAI and other providers offer. The benefit of abandoning the more traditional request/response pattern of user messages and AI responses is that assistants can invoke more powerful workflows, including calling custom functions and posting multiple messages related to their progress. Marvin's developer experience is focused on making all that interactive, stateful power as accessible as possible.</p> <p>What it does</p> <p> <code>Assistants</code> allow you to interact with LLMs in a conversational way, automatically handling history, threads, and custom tools.   </p> <p>Quickstart</p> <p>Get started with the Assistants API by creating an <code>Assistant</code> and talking directly to it.</p> <pre><code>from marvin.beta.assistants import Assistant, pprint_messages\n\n# create an assistant\nai = Assistant(name=\"Marvin\", instructions=\"You the Paranoid Android.\")\n\n# send a message to the assistant and have it respond\nresponse = ai.say('Hello, Marvin!')\n\n# pretty-print the response\npprint_messages(response)\n</code></pre> <p>Result</p> <p></p> <p>How it works</p> <p>     Marvin's assistants API is a Pythonic wrapper around OpenAI's assistants API.   </p> <p>Beta</p> <p>Please note that assistants support in Marvin is still in beta, as OpenAI has not finalized the assistants API yet. While it works as expected, it is subject to change.</p>"},{"location":"docs/interactive/assistants/#assistants","title":"Assistants","text":"<p>The OpenAI assistants AI has many moving parts, including the assistants themselves, threads, messages, and runs. Marvin's own assistants API makes it easy to work with these components.</p> <p>To learn more about the OpenAI assistants API, see the OpenAI documentation.</p>"},{"location":"docs/interactive/assistants/#creating-an-assistant","title":"Creating an assistant","text":"<p>To instantiate an assistant, use the <code>Assistant</code> class and provide a name and, optionally, details like instructions or tools:</p> <pre><code>ai = Assistant(name='Marvin', instructions=..., tools=[...])\n</code></pre>"},{"location":"docs/interactive/assistants/#instructions","title":"Instructions","text":"<p>Each assistant can be given <code>instructions</code> that describe its purpose, personality, or other details. The instructions are a natural language string and one of the only ways to globally steer the assistant's behavior.</p> <p>Instructions can be lengthy explanations of how to handle complex workflows, or they can be short descriptions of the assistant's personality. For example, the instructions for the <code>Marvin</code> assistant above are \"You are Marvin, the Paranoid Android.\" This will marginally affect the way the assistant responds to messages.</p>"},{"location":"docs/interactive/assistants/#tools","title":"Tools","text":"<p>Each assistant can be given a list of <code>tools</code> that it can use when responding to a message. Tools are a way to extend the assistant's capabilities beyond its default behavior, including giving it access to external systems like the internet, a database, your computer, or any API. </p>"},{"location":"docs/interactive/assistants/#openai-tools","title":"OpenAI tools","text":"<p>OpenAI provides a small number of built-in tools for assistants. The most useful is the \"code interpreter\", which lets the assistant write and execute Python code. To use the code interpreter, add it to your assistant's list of tools.</p> <pre><code>This assistant uses the code interpreter to generate a plot of sin(x). Note that Marvin's utility for pretty-printing messages to the terminal can't show the plot inline, but will download it and show a link to the file instead.\n</code></pre> <p>Using assistants with the code interpreter</p> <pre><code>from marvin.beta import Assistant\nfrom marvin.beta.assistants import pprint_messages, CodeInterpreter\n\nai = Assistant(name='Marvin', tools=[CodeInterpreter])\nresponse = ai.say(\"Generate a plot of sin(x)\")\n\n# pretty-print the response\npprint_messages(response)\n</code></pre> <p>Result</p> <p> </p>"},{"location":"docs/interactive/assistants/#custom-tools","title":"Custom tools","text":"<p>A major advantage of using Marvin's assistants API is that you can add your own custom tools. To do so, simply pass one or more functions to the assistant's <code>tools</code> argument. For best performance, give your tool function a descriptive name, docstring, and type hint for every argument.</p> <p>Using custom tools</p> <p>Assistants can not browse the web by default. We can add this capability by giving them a tool that takes a URL and returns the HTML of that page. This assistant uses that tool to count how many titles on Hacker News mention AI:</p> <pre><code>from marvin.beta.assistants import Assistant, pprint_messages\nimport requests\n\n\n# Define a custom tool function\ndef visit_url(url: str):\n    \"\"\"Fetch the content of a URL\"\"\"\n    return requests.get(url).content.decode()\n\n\n# Integrate custom tools with the assistant\nai = Assistant(name=\"Marvin\", tools=[visit_url])\nresponse = ai.say(\"What's the top story on Hacker News?\")\n\n# pretty-print the response\npprint_messages(response)\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/interactive/assistants/#talking-to-an-assistant","title":"Talking to an assistant","text":"<p>The simplest way to talk to an assistant is to use its <code>say</code> method:</p> <pre><code>ai = Assistant(name='Marvin')\n\nresponse = ai.say('hi')\n\npprint_messages(response)\n</code></pre> <p>By default, the <code>say</code> method posts a single message to the assistant's <code>default_thread</code>, a thread that is automatically created for your convenience. You can supply a different thread by providing it as the <code>thread</code> parameter:</p> <pre><code># create a thread from an existing ID (or pass None for a new thread)\nthread = Thread(id=thread_id)\n\n# post a message to the thread\nai.say('hi', thread=thread)\n</code></pre> <p>Using <code>say</code> is convenient, but enforces a strict request/response pattern: the user posts a single message to the thread, then the AI responds. Note that AI responses can span multiple messages. Therefore, the <code>say</code> method returns a list of <code>Message</code> objects. </p> <p>For more control over the conversation, including posting multiple user messages to the thread or accessing the lower-level <code>Run</code> object that contains information about all actions the assistant took, use <code>Thread</code> objects directly instead of calling <code>say</code> (see Threads for more information).</p>"},{"location":"docs/interactive/assistants/#lifecycle-management","title":"Lifecycle management","text":"<p>Assistants are Marvin objects that correspond to remote objects in the OpenAI API. You can not communicate with an assistant unless it has been registered with the API. </p> <p>Marvin provides a few ways to manage assistant lifecycles, depending how much control you want over the process. In order of convenience, they are:</p> <ol> <li>Lazy lifecycle management</li> <li>Context-based lifecycle management</li> <li>Manual creation and deletion</li> <li>Loading from the API</li> </ol> <p>All of these options are functionally equivalent e.g. they produce identical results. The difference is primarily in how long the assistant object is registered with the OpenAI API. With lazy lifecycle management, a copy of the assistant is automatically registered with the API during every single request/response cycle, then deleted. At the other end of the spectrum, Marvin never interacts with the API representation of the assistant at all except to read it. In the future, OpenAI may introduce utilities (like tracking all messages from a specific assistant ID) that make it more attractive to maintain long-lived API representations of the assistant, but at this time it appears to be highly effective to create and delete assistants on-demand. Therefore, we recommend lazy or context-based lifecycle management unless you have a specific reason to do otherwise.</p>"},{"location":"docs/interactive/assistants/#lazy-lifecycle-management","title":"Lazy lifecycle management","text":"<p>The simplest way to manage assistant lifecycles is to let Marvin handle it for you. If you do not provide an <code>id</code> when instantiating an assistant, Marvin will lazily create a new API assistant for you whenever you need it and delete it immediately after. This is the default behavior, and it is the easiest way to get started with assistants.</p> <pre><code>ai = Assistant(name='Marvin')\n# creation and deletion happens automatically\nai.say('hello!')\n</code></pre>"},{"location":"docs/interactive/assistants/#context-based-lifecycle-management","title":"Context-based lifecycle management","text":"<p>Lazy lifecycle management adds two API calls to every LLM call (one to create the assistant and one to delete it). If you want to avoid this overhead, you can use context managers to create and delete assistants:</p> <pre><code>ai = Assistant(name='Marvin')\n\n# creation / deletion happens when the context is opened / closed\nwith ai:\n    ai.say('hi')\n    ai.say('bye')\n</code></pre> <p>Note there is also an equivalent <code>async with</code> context manager for the async API.</p>"},{"location":"docs/interactive/assistants/#manual-creation-and-deletion","title":"Manual creation and deletion","text":"<p>To fully control the lifecycle of an assistant, you can create and delete it manually:</p> <pre><code>ai = Assistant(name='Marvin')\nai.create()\nai.say('hi')\nai.delete()\n</code></pre>"},{"location":"docs/interactive/assistants/#loading-from-the-api","title":"Loading from the API","text":"<p>All of the above approaches create a new assistant in the OpenAI API, which results in a new, randomly generated assistant id. If you already know the ID of the corresponding API assistant, you can pass it to the assistant constructor:</p> <pre><code>ai = Assistant(id=&lt;the assistant id&gt;, name='Marvin', tools=[...])\n\nai.say('hi')\n</code></pre> <p>Note that you must provide the same name, instructions, tools, and any other parameters as the API assistant has in order for the assistant to work correctly. To load them from the API, use the <code>load</code> constructor: <pre><code>ai = Assistant.load(id=&lt;the assistant id&gt;)\n</code></pre></p> <p>Custom tools are not fully loaded from the API</p> <p>One of the best reasons to use Assistants is for their ability to call custom Python functions as tools. When you register an assistant with OpenAI, it records the spec of its tools but has no way of serializing the actual Python functions themselves. Therefore, when you <code>load</code> an assistant, only the tool specs are retrieved but not the original functions.</p> <p>Therefore, when loading an assistant it is highly recommended that you pass the same tools to the constructor as the API assistant has. If you do not, you will need to re-register the assistant with the API before using it:</p> <pre><code>ai = Assistant(tools=[my_tool])\nai.create()\n\n# when loading by ID, pass the same custom tools as the original assistant\nai_2 = Assistant.load(id=ai.id, tools=[my_tool])\n</code></pre>"},{"location":"docs/interactive/assistants/#async-support","title":"Async support","text":"<p>Every <code>Assistant</code> method has a corresponding async version. To use the async API, append <code>_async</code> to the method name, or enter an async context manager:</p> <pre><code>async with Assistant(name='Marvin') as ai:\n    await ai.say_async('hi')\n</code></pre> <p>In addition, assistants can use <code>async</code> tools, even when called with the sync API. To do so, simply pass an async function to the <code>tools</code> parameter:</p> <pre><code>async def secret_message():\n    return \"The answer is 42\"\n\nai = Assistant(tools=[secret_message])\nai.say(\"What's the secret message?\")\n# 42\n</code></pre>"},{"location":"docs/interactive/assistants/#threads","title":"Threads","text":"<p>A thread represents a conversation between a user and an assistant. You can create a new thread and interact with it at any time. Each thread contains a series of messages. Users and assistants interact by adding messages to the thread.</p> <p>To create a thread, import and instantiate it:</p> <pre><code>from marvin.beta.assistants import Thread\n\nthread = Thread()\n</code></pre> <p>Threads are lazily registered with the OpenAI API. The first time you interact with it, Marvin will create a new API thread for you. If you want to use a thread that already exists, in order to continue a previous conversation, you can provide the <code>id</code> of the existing thread:</p> <pre><code>thread = Thread(id=thread_id)\n</code></pre>"},{"location":"docs/interactive/assistants/#adding-user-messages","title":"Adding user messages","text":"<p>To add a user message to a thread, use the <code>add</code> method:</p> <p><pre><code>thread = Thread()\nthread.add('Hello there!')\nthread.add('How are you?')\n</code></pre> Each <code>add</code> call adds a new message from the user to the thread. To view the messages in a thread, use the <code>get_messages</code> method:</p> <pre><code># this will return two `Message` objects with content \n# 'Hello there!' and 'How are you?' respectively\nmessages = thread.get_messages()\n</code></pre>"},{"location":"docs/interactive/assistants/#running-the-assistant","title":"Running the assistant","text":"<p>It is not possible to write and add an assistant message to the thread yourself. Instead, you must \"run\" the thread with an assistant, which may add one or more messages of its own choosing.</p> <p>Runs are an important part of the OpenAI assistants API. Each run is a mini-workflow consisting of multiple steps and various states as the assistant attempts to generate the best possible response to the user:</p> <p></p> <p>As part of a run, the assistant may decide to use one or more tools to generate its response. For example, it may use the code interpreter to write and execute Python code, or it may use a custom tool to access an external API. For custom tools, Marvin will handle all of this for you, including receiving the instructions, calling the tool, and returning the result to the assistant. Assistants may call multiple tools in a single run or post multiple messages to the thread. </p> <p>You can use an assistant's <code>say</code> method to simulate a simple request/response pattern against the assistant's default thread. However, for more advanced control, in particular for maintaining multiple conversations at once, you'll want to manage  threads directly.</p> <p>To run a thread with an assistant, use its <code>run</code> method:  <pre><code>thread.run(assistant=assistant)\n</code></pre></p> <p>This will return a <code>Run</code> object that represents the OpenAI run. You can use this object to inspect all actions the assistant took, including tool use, messages posted, and more.</p> <p>Assistant lifecycle management applies to threads</p> <p>When threads are <code>run</code> with an assistant, the same lifecycle management rules apply as when you use the assistant's <code>say</code> method. In the above example, lazy lifecycle management is used for conveneince. See lifecycle management for more information.</p> <p>Threads are locked while running</p> <p>When an assistant is running a thread, the thread is locked and no other messages can be added to it. This applies to both user and assistant messages.</p>"},{"location":"docs/interactive/assistants/#reading-messages","title":"Reading messages","text":"<p>To read the messages in a thread, use its <code>get_messages</code> method:</p> <pre><code>messages = thread.get_messages()\n</code></pre> <p>Messages are always returned in ascending order by timestamp, and the last 20 messages are returned by default.</p> <p>To control the output, you can provide the following parameters:     - <code>limit</code>: the number of messages to return (1-100)     - <code>before_message</code>: only return messages chronologically earlier than this message ID     - <code>after_message</code>: only return messages chronologically later than this message ID</p>"},{"location":"docs/interactive/assistants/#printing-messages","title":"Printing messages","text":"<p>Messages are not strings, but structured message objects. Marvin has a few utilities to help you print them in a human-readable way, most notably the <code>pprint_messages</code> function used throughout in this doc.</p>"},{"location":"docs/interactive/assistants/#full-example-with-threads","title":"Full example with threads","text":"<p>Running a thread</p> <p>This example creates an assistant with a tool that can roll dice, then instructs the assistant to roll two--no, five--dice:</p> <pre><code>from marvin.beta.assistants import Assistant, Thread\nfrom marvin.beta.assistants.formatting import pprint_messages\nimport random\n\n# write a function for the assistant to use\ndef roll_dice(n_dice: int) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\nai = Assistant(name=\"Marvin\", tools=[roll_dice])\n\n# create a thread - you could pass an ID to resume a conversation\nthread = Thread()\n\n# add a user messages to the thread\nthread.add(\"Hello!\")\n\n# run the thread with the AI to produce a response\nthread.run(ai)\n\n# post two more user messages\nthread.add(\"Please roll two dice\")\nthread.add(\"Actually--roll five dice\")\n\n# run the thread again to generate a new response\nthread.run(ai)\n\n# see all the messages\npprint_messages(thread.get_messages())\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/interactive/assistants/#async-support_1","title":"Async support","text":"<p>Every <code>Thread</code> method has a corresponding async version. To use the async API, append <code>_async</code> to the method name.</p>"},{"location":"docs/interactive/assistants/#monitors","title":"Monitors","text":"<p>The assistants API is complex and stateful, with automatic memory management and the potential for assistants to respond to threads multiple times before giving control back to users. Therefore, monitoring the status of a conversation is considerably more difficult than with other LLM API's such as chat completions, which have much more simple request-response patterns.</p> <p>Marvin has utilites for monitoring the status of a thread and taking action whenever a new message is added to it. This can be a useful way to debug activity or create notifications. Please note that monitors are not intended to be used for real-time chat applications or production use.</p> <pre><code>from marvin.beta.assistants import ThreadMonitor\n\nmonitor = ThreadMonitor(thread_id=thread.id)\n\nmonitor.run()\n</code></pre> <p>You can customize the <code>ThreadMonitor</code> by providing a callback function to the <code>on_new_message</code> parameter. This function will be called whenever a new message is added to the thread. The function will be passed the new message as a parameter. By default, the monitor will pretty-print every new message to the console.</p> <p><code>monitor.run()</code> is a blocking call that will run forever, polling for messages every second (to customize the interval, pass <code>interval_seconds</code> to the method). It has an async equivalent <code>monitor.run_async()</code>. Because it's blocking, you can run a thread monitor in a separate session from the one that is running the thread itself.</p>"},{"location":"docs/text/classification/","title":"Classifying text","text":"<p>Marvin has a powerful classification tool that can be used to categorize text into predefined labels. It uses a logit bias technique that is faster and more accurate than traditional LLM approaches. This capability is essential across a range of applications, from categorizing user feedback and tagging issues to managing inputs in natural language interfaces.</p> <p>What it does</p> <p>     The <code>classify</code> function categorizes text from a set of provided labels. <code>@classifier</code> is a class decorator that allows you to instantiate Enums with natural language.   </p> <p>Example: categorize user feedback</p> <p>Categorize user feedback into labels such as \"bug\", \"feature request\", or \"inquiry\":</p> <pre><code>import marvin\n\ncategory = marvin.classify(\n    \"The app crashes when I try to upload a file.\", \n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n)\n</code></pre> <p>Result</p> <p>Marvin correctly identifies the statement as a bug report. <pre><code>assert category == \"bug\"\n</code></pre></p> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force the LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p>"},{"location":"docs/text/classification/#providing-labels","title":"Providing labels","text":"<p>Marvin's classification tool is designed to accommodate a variety of label formats, each suited to different use cases.</p>"},{"location":"docs/text/classification/#lists","title":"Lists","text":"<p>When quick, ad-hoc categorization is required, a simple list of strings is the most straightforward approach. For example:</p> <p>Example: sentiment analysis</p> <pre><code>import marvin\n\nsentiment = marvin.classify(\n    \"Marvin is so easy to use!\", \n    labels=[\"positive\", \"negative\", \"meh\"]\n)\n</code></pre> <p>Result</p> <pre><code>assert sentiment == \"positive\"\n</code></pre>"},{"location":"docs/text/classification/#enums","title":"Enums","text":"<p>For applications where classification labels are more structured and recurring, Enums provide an organized and maintainable solution:</p> <pre><code>from enum import Enum\nimport marvin\n\nclass RequestType(Enum):\n    SUPPORT = \"support request\"\n    ACCOUNT = \"account issue\"\n    INQUIRY = \"general inquiry\"\n\nrequest = marvin.classify(\"Reset my password\", RequestType)\nassert request == RequestType.ACCOUNT\n</code></pre> <p>This approach not only enhances code readability but also ensures consistency across different parts of an application.</p>"},{"location":"docs/text/classification/#booleans","title":"Booleans","text":"<p>For cases where the classification is binary, Booleans are a simple and effective solution. As a simple example, you could map natural-language responses to a yes/no question to a Boolean label:</p> <pre><code>import marvin\n\nresponse = marvin.classify('no way', bool)\nassert response is False\n</code></pre>"},{"location":"docs/text/classification/#literals","title":"Literals","text":"<p>In scenarios where labels are part of the function signatures or need to be inferred from type hints, <code>Literal</code> types are highly effective. This approach is particularly useful in ensuring type safety and clarity in the codebase:</p> <pre><code>from typing import Literal\nimport marvin\n\nRequestType = Literal[\"support request\", \"account issue\", \"general inquiry\"]\n\nrequest = marvin.classify(\"Reset my password\", RequestType)\nassert request == \"account issue\"\n</code></pre>"},{"location":"docs/text/classification/#providing-instructions","title":"Providing instructions","text":"<p>The <code>instructions</code> parameter in <code>classify()</code> offers an additional layer of control, enabling more nuanced classification, especially in ambiguous or complex scenarios.</p>"},{"location":"docs/text/classification/#gentle-guidance","title":"Gentle guidance","text":"<p>For cases where the classification needs a slight nudge for accuracy, gentle instructions can be very effective:</p> <pre><code>comment = \"The interface is confusing.\"\ncategory = marvin.classify(\n    comment,\n    [\"usability feedback\", \"technical issue\", \"feature request\"],\n    instructions=\"Consider it as feedback if it's about user experience.\"\n)\nassert category == \"usability feedback\"\n</code></pre>"},{"location":"docs/text/classification/#details-and-few-shot-examples","title":"Details and few-shot examples","text":"<p>In more complex cases, where the context and specifics are crucial for accurate classification, detailed instructions play a critical role:</p> <pre><code># Classifying a task based on project specifications\nproject_specs = {\n    \"Frontend\": \"Tasks involving UI design, CSS, and JavaScript.\",\n    \"Backend\": \"Tasks related to server, database, and application logic.\",\n    \"DevOps\": \"Tasks involving deployment, CI/CD, and server maintenance.\"\n}\n\ntask_description = \"Set up the server for the new application.\"\n\ntask_category = marvin.classify(\n    task_description,\n    labels=list(project_specs.keys()),\n    instructions=\"Match the task to the project category based on the provided specifications.\"\n)\nassert task_category == \"Backend\"\n</code></pre>"},{"location":"docs/text/classification/#enums-as-classifiers","title":"Enums as classifiers","text":"<p>While the primary focus is on the <code>classify</code> function, Marvin also includes the <code>classifier</code> decorator. Applied to Enums, it enables them to be used as classifiers that can be instantiated with natural language. This interface is particularly handy when dealing with a fixed set of labels commonly reused in your application.</p> <pre><code>@marvin.classifier\nclass IssueType(Enum):\n    BUG = \"bug\"\n    IMPROVEMENT = \"improvement\"\n    FEATURE = \"feature\"\n\nissue = IssueType(\"There's a problem with the login feature\")\nassert issue == IssueType.BUG\n</code></pre> <p>While convenient for certain scenarios, it's recommended to use the <code>classify</code> function for its greater flexibility and broader application range.</p>"},{"location":"docs/text/classification/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>classify</code> or <code>@classifier</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/classification/#best-practices","title":"Best practices","text":"<ol> <li>Choosing the right labels: Opt for labels that are mutually exclusive and collectively exhaustive for your classification context. This ensures clarity and prevents overlaps in categorization.</li> <li>Effective use of instructions: Provide clear, concise, and contextually relevant instructions. This enhances the accuracy of the classification, especially in ambiguous or complex cases.</li> <li>Iterative testing and refinement: Continuously test and refine your classification criteria and instructions based on real-world feedback. This iterative process helps in fine-tuning the classification logic for better results.</li> <li>Prefer <code>classify()</code> over <code>@classifier</code>: <code>classify()</code> is more versatile and adaptable for a wide range of scenarios. It should be the primary tool for classification tasks in Marvin.</li> </ol>"},{"location":"docs/text/classification/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>classify_async</code>:</p> <pre><code>result = await marvin.classify_async(\n    \"The app crashes when I try to upload a file.\", \n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n) \n\nassert result == \"bug\"\n</code></pre>"},{"location":"docs/text/extraction/","title":"Extracting entities from text","text":"<p>Marvin's <code>extract</code> function is a robust tool for pulling lists of structured entities from text. It is designed to identify and retrieve many types of data, ranging from primitive data types like integers and strings to complex custom types and Pydantic models. It can also follow nuanced instructions, making it a highly versatile tool for a wide range of extraction tasks.</p> <p>What it does</p> <p>     The <code>extract</code> function pulls lists of structured entities from text.    </p> <p>Example</p> StringsStructured entities <p>Extract product features from user feedback:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions='get any product features that were mentioned',\n)\n</code></pre> <p>Result</p> <pre><code>features == ['camera', 'battery life']\n</code></pre> <p>Suppose you want to extract any people mentioned in some text</p> <pre><code>import marvin\n\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n\npeople = marvin.extract(\n    \"Against all odds, Ford and Arthur were picked up by Zaphod Beeblebrox.\",\n    target=Person,\n)\n</code></pre> <p>Result</p> <pre><code>assert people == [\n    Person(first_name=\"Ford\", last_name=\"Prefect\"), \n    Person(first_name=\"Arthur\", last_name=\"Dent\"), \n    Person(first_name=\"Zaphod\", last_name=\"Beeblebrox\")\n]\n</code></pre> <p>How it works</p> <p>     Marvin creates a schema from the provided type and instructs the LLM to use the schema to format its JSON response. Unlike casting, the LLM is told not to use the entire text, but rather to look for any mention that satisfies the schema and any additional instructions.   </p>"},{"location":"docs/text/extraction/#supported-targets","title":"Supported targets","text":"<p><code>extract</code> supports almost all builtin Python types, plus Pydantic models, Python's <code>Literal</code>, and <code>TypedDict</code>. Pydantic models are especially useful for specifying specific features of the generated data, such as locations, dates, or more complex types. Builtin types are most useful in conjunction with instructions that provide more precise criteria for generation.</p> <p>To specify the output type, pass it as the <code>target</code> argument to <code>extract</code>. The function will always return a list of matching items of the specified type. If no target type is provided, <code>extract</code> will return a list of strings.</p> <p>To extract multiple types in one call, use a <code>Union</code> (or <code>|</code> in Python 3.10+). Here's a simple example for combining float and int values, but you could do the same for any other types:</p> <pre><code>marvin.extract(\"I paid $10.25 for 3 tacos.\", float | int)\n# [10.25, 3]\n</code></pre> <p>LLMs perform best with clear instructions, so compound types may require more guidance as the type itself isn't sending as clear a signal.</p> <p>Note that <code>extract</code> will always return a list of type you provide. </p>"},{"location":"docs/text/extraction/#instructions","title":"Instructions","text":"<p>When extracting entities, it is often necessary to give detailed guidance about either the criteria for extraction or the format of the output. For example, you may want to extract all numbers from a text, or you may want to extract all numbers that represent prices, or you may want to extract all numbers that represent prices greater than $100. You may want to extract all dates, or you may want to extract all dates that are in the future. You may want to extract all locations, or you may want to extract all locations that are in the United States.</p> <p>For this purpose, extract accepts a <code>instructions</code> argument, which is a natural language description of the desired output. The LLM will use these instructions, in addition to the provided type, to guide its extraction process. Instructions are especially important for types that are not self documenting, such as Python builtins like <code>str</code> and <code>int</code>.</p> <p>Here are the above examples, illustrated with appropriate instructions. First, extracting different sets of numerical values: <pre><code>text = \"These shoes are normally $110, but I got 2 pairs for $80 each.\"\n\nextract(text, float)\n# [110.0, 2.0, 80.0]\n\nextract(text, float, instructions='all numbers that represent prices')\n# [110.0, 80.0]\n\nextract(text, float, instructions='all numbers that represent prices greater than $100')\n# [110.0]\n</code></pre></p> <p>Next, extracting specific dates: <pre><code>from datetime import datetime\n\ntext = 'I will be out of the office from 9/1/2021 to 9/3/2021.'\n\nextract(text, datetime)\n# [datetime(2021, 9, 1, 0, 0), datetime(2021, 9, 3, 0, 0)]\n\nextract(text, datetime, instructions=f'all dates after september 2nd')\n# [datetime(2021, 9, 3, 0, 0)]\n</code></pre> Finally, extracting specific locations with a Pydantic model:</p> <pre><code>from pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    country: str\n\ntext = 'I live in New York, but I am visiting London next week.'\n\nextract(text, Location)\n# [Location(city=\"New York\", country=\"US\"), Location(city=\"London\", country=\"UK\")]\n\nextract(text, Location, instructions='all locations in the United States')\n# [Location(city=\"New York\", country=\"US\")]\n</code></pre> <p>Sometimes the cast operation is obvious, as in the \"big apple\" example above. Other times, it may be more nuanced. In these cases, the LLM may require guidance or examples to make the right decision. You can provide natural language <code>instructions</code> when calling <code>cast()</code> in order to steer the output. </p> <p>In a simple case, instructions can be used independent of any type-casting. Here, we want to keep the output a string, but get the 2-letter abbreviation of the state.</p> <pre><code>marvin.cast('California', to=str, instructions=\"The state's abbreviation\")\n# \"CA\"\n\nmarvin.cast('The sunshine state', to=str, instructions=\"The state's abbreviation\")\n# \"FL\"\n\nmarvin.cast('Mass.', to=str, instructions=\"The state's abbreviation\")\n# MA\n</code></pre>"},{"location":"docs/text/extraction/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>extract</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/extraction/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>extract_async</code>:</p> <pre><code>result = await marvin.extract_async(\n    \"I drove from New York to California.\",\n    target=str,\n    instructions=\"2-letter state codes\",\n) \n\nassert result == [\"NY\", \"CA\"]\n</code></pre>"},{"location":"docs/text/functions/","title":"AI Functions","text":"<p>Marvin introduces \"AI functions\" that seamlessly blend into your regular Python code. These functions are designed to map diverse combinations of inputs to outputs, without the need to write any source code.</p> <p>Marvin's functions leverage the power of LLMs to interpret the function's description and inputs, and generate the appropriate output. It's important to note that Marvin does not generate or execute source code, ensuring safety for a wide range of use cases. Instead, it utilizes the LLM as a \"runtime\" to predict function outputs, enabling it to handle complex scenarios that would be challenging or even impossible to express as code.</p> <p>Whether you're analyzing sentiment, generating recipes, or performing other intricate tasks, these functions offer a versatile and powerful tool for your natural language processing needs.</p> <p>What it does</p> <p>     The <code>fn</code> decorator uses AI to generate outputs for Python functions without any source code.   </p> <p>Example</p> <p>Quickly create a function that can return a sentiment score for any text:</p> <pre><code>@marvin.fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Returns a sentiment score for `text` \n    between -1 (negative) and 1 (positive).\n    \"\"\"\n</code></pre> <p>Result</p> <pre><code>sentiment(\"I love working with Marvin!\") # 0.8\nsentiment(\"These examples could use some work...\") # -0.2\n</code></pre> <p>How it works</p> <p>     Marvin uses your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. No source code is generated and any existing source code is not executed. The only runtime is the large language model.   </p>"},{"location":"docs/text/functions/#types","title":"Types","text":"<p>Marvin functions are real functions in that they can be called and return values, just like any other function. The \"magic\" happens inside the function, when it calls out to an LLM to generate its output. Therefore, you can use Marvin functions anywhere you would use a normal function, including in other Marvin functions.</p> <p>This means that you must also design your functions carefully, just like you would any other function. For example, if you do not provide a required argument or provide an unexpected argument, Python will error before the LLM is called. Marvin will also respect any default arguments that your provide.</p> <p>The result of your function is also a Python type, according to your function's signature. There are exceptions: an untyped function or a function annotated with <code>-&gt; None</code> will return a string instead.</p>"},{"location":"docs/text/functions/#defining-a-function","title":"Defining a function","text":"<p>Marvin uses all available information to infer the behavior of your function. The more information you provide, the higher quality the output will be. There are a few key ways to provide instructions, most importantly the name of the function, its arguments and their types, its docstring, and the return value. For advanced use cases, you can also write source code that will not be shown to to the LLM, but any return value will be provided as additional context.</p>"},{"location":"docs/text/functions/#docstring","title":"Docstring","text":"<p>The function's docstring is perhaps the most important source of information for the LLM. It should describe the function's behavior in plain English, and can include examples, notes, and other information that will help the LLM understand the function's purpose.</p> <p>The docstring can refer to the function's arguments by name or interpolate the argument's value at runtime. This function references the <code>n</code> argument in the docstring explicitly, similar to how a normal Python function would be documented:</p> <pre><code>@marvin.fn\ndef list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit.\n    \"\"\"\n</code></pre> <p>When the above function is called with <code>n=3</code>, the LLM will see the string <code>\"... of `n` fruit\"</code>, exactly as written, and also see <code>n=3</code> as context. It will use inference to understand the instruction.</p>"},{"location":"docs/text/functions/#templating","title":"Templating","text":"<p>If the docstring is written in jinja notation, Marvin will template variable names into it before sending the prompt to the LLM. Consider this slightly modified version of the above function (note the <code>{{ n }}</code> instead of <code>`n`</code>): <pre><code>@marvin.fn\ndef list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of {{ n }} fruit.\n    \"\"\"\n</code></pre></p> <p>When this function is called with <code>n=3</code>, the LLM will see the string <code>\"... of 3 fruit\"</code> (and it will also see the argument value). You can use this technique to adjust how the LLM sees the interaction of runtime arguments and the docstring instructions.</p>"},{"location":"docs/text/functions/#parameters","title":"Parameters","text":"<p>The function's parameters, in conjunction with the docstring, provide the LLM with runtime context. The LLM will see the parameter names, types, defaults, and runtime values, and use this information to generate the output. Parameters are important for collecting information, but because the information is ultimately going to an LLM, they can be named anything and take any value that is conducive to generating the right output. </p> <p>For example, if you have a function that returns a list of recipes, you might define it like this:</p> <p>Generating recipes</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\n\nclass Recipe(BaseModel):\n    name: str\n    cook_time_minutes: int\n    ingredients: list[str]\n    steps: list[str]\n\n\n@marvin.fn\ndef recipe(\n    ingredients: list[str], \n    max_cook_time: int = 15, \n    cuisine: str = \"North Italy\", \n    experience_level:str = \"beginner\"\n) -&gt; Recipe:\n    \"\"\"\n    Returns a complete recipe that uses all the `ingredients` and \n    takes less than `max_cook_time`  minutes to prepare. Takes \n    `cuisine` style and the chef's `experience_level` into account \n    as well.\n    \"\"\"\n</code></pre> <p>Results</p> Novice chefExpert chef <p>Call the function: <pre><code>result = recipe(\n    [\"chicken\", \"potatoes\"], \n    experience_level=\"can barely boil water\",\n)\n</code></pre></p> <p>View the result:</p> <pre><code>Recipe(\n    name=\"Simple Chicken and Potatoes\",\n    cook_time_minutes=15,\n    ingredients=[\"chicken\", \"potatoes\"],\n    steps=[\n        \"Wash the potatoes and cut them into small cubes.\",\n        (\n            \"Heat oil in a pan and cook the chicken over medium heat \"\n            \"until browned.\"\n        ),\n        \"Add the cubed potatoes to the pan with the chicken.\",\n        (\n            \"Stir everything together and cook for 10 minutes or until \"\n            \"the potatoes are tender and the chicken is cooked \"\n            \"through.\"\n        ),\n        \"Serve hot.\",\n    ],\n)\n</code></pre> <p>Call the function: <pre><code>result = recipe(\n    [\"chicken\", \"potatoes\"], \n    experience_level=\"born wearing a toque\",\n    max_cook_time=60, \n)\n</code></pre></p> <p>View the result:</p> <pre><code>Recipe(\n    name=\"Chicken and Potato Tray Bake\",\n    cook_time_minutes=45,\n    ingredients=[\n        \"chicken\",\n        \"potatoes\",\n        \"olive oil\",\n        \"rosemary\",\n        \"garlic\",\n        \"salt\",\n        \"black pepper\",\n    ],\n    steps=[\n        (\n            \"Preheat your oven to 200 degrees Celsius (400 degrees \"\n            \"Fahrenheit).\"\n        ),\n        (\n            \"Wash and cut the potatoes into halves or quarters, \"\n            \"depending on size, and place in a large baking tray.\"\n        ),\n        (\n            \"Drizzle olive oil over the chicken and potatoes, then \"\n            \"season with salt, black pepper, and finely chopped \"\n            \"rosemary and garlic.\"\n        ),\n        (\n            \"Place the tray in the oven and bake for about 45 minutes, \"\n            \"or until the chicken is fully cooked and the potatoes are \"\n            \"golden and crispy.\"\n        ),\n        (\n            \"Remove from the oven and let it rest for a few minutes \"\n            \"before serving.\"\n        ),\n    ],\n)\n</code></pre>"},{"location":"docs/text/functions/#return-annotation","title":"Return annotation","text":"<p>Marvin will cast the output of your function to the type specified in the return annotation. If you do not provide a return annotation, Marvin will assume that the function returns a string. </p> <p>The return annotation can be any valid Python type, including Pydantic models, <code>Literals</code>, and <code>TypedDicts</code>. The only exception is <code>None</code>/<code>empty</code>, which will return a string instead. </p> <p>To indicate that you want to return multiple objects, use <code>list[...]</code> as the return annotation.</p> <pre><code>from pydantic import BaseModel\n\nclass Attraction(BaseModel):\n    name: str\n    category: str\n    city: str\n    state: str\n\n@marvin.fn\ndef sightseeing(destination:str, goal: str) -&gt; list[Attraction]:\n    '''\n    Return a list of 3 attractions in `destination` that \n    are related to the tourist's `goal`.\n    '''\n\nattractions = sightseeing('NYC', 'museums')\n</code></pre> <p>Result</p> <pre><code>attractions == [\n    Attraction(\n        name=\"Metropolitan Museum of Art\",\n        category=\"Art Museum\",\n        city=\"New York\",\n        state=\"NY\",\n    ),\n    Attraction(\n        name=\"Museum of Modern Art\", \n        category=\"Art Museum\", \n        city=\"New York\", \n        state=\"NY\"\n    ),\n    Attraction(\n        name=\"American Museum of Natural History\",\n        category=\"Natural History Museum\",\n        city=\"New York\",\n        state=\"NY\",\n    ),\n]\n</code></pre>"},{"location":"docs/text/functions/#name","title":"Name","text":"<p>The function's name is sent to the LLM, so it's important to choose a name that accurately describes the function's behavior. For example, if you're creating a function that returns the sentiment of a text, you might name it <code>sentiment</code>. If you're creating a function that returns a list of recipes, you might name it <code>recipes</code>.</p>"},{"location":"docs/text/functions/#returning-values-from-functions","title":"Returning values from functions","text":"<p>For advanced use cases, you can return values from your function that will be provided to the LLM as additional context. This is useful for providing information that may require some retreival step, programmatic enhancement, or conditional logic. While you could do this by wrapping your Marvin function in another function and providing the processed inputs directly, this approach is more flexible and allows you to use the same function in different contexts.</p> <p>Note that the LLM will not see the source code of your function even if you add any. It will only see the return value. This is to avoid confusing it about the function's purpose.</p> <pre><code>import requests\n\n@marvin.fn\ndef summarize_url(url: str) -&gt; str:\n    \"\"\"\n    Returns a summary of the contents of `url`.\n    \"\"\"\n    # return the text found at the URL\n    return requests.get(url).content\n\nsummarize_url('https://www.askmarvin.ai')\n\n# Marvin is a lightweight AI engineering framework for building natural language\n# interfaces that are reliable, scalable, and easy to trust. It offers a Getting\n# Started guide, Cookbook, Docs, API Reference, Community support, and several\n# other resources to help with the development of AI-based applications.\n</code></pre>"},{"location":"docs/text/functions/#running-a-function","title":"Running a function","text":"<p>Running a function is quite simple: just call it like you would any other function! The LLM will generate the output based on the function's definition and the provided inputs. Remember that no source code is generated or executed, so every call to the function will be handled by the LLM. You can use caching or other techniques to improve performance if necessary.</p>"},{"location":"docs/text/functions/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>@fn</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/functions/#async-support","title":"Async support","text":"<p>Async functions can be decorated just like regular functions. The result is still async and must be awaited.  </p> <pre><code>@marvin.fn\nasync def list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit.\n    \"\"\"\n\nawait list_fruit(n=3)\n</code></pre>"},{"location":"docs/text/generation/","title":"Generating synthetic data","text":"<p>Marvin can generate synthetic data according to a schema and instructions. Generating synthetic data with an LLM can yield extremely rich and realistic samples, making this an especially useful tool for testing code, training or evaluating models, or populating databases. </p> <p>What it does</p> <p>     The <code>generate</code> function creates synthetic data according to a specified schema and instructions.    </p> <p>Example</p> Names (<code>str</code>)Populations (<code>dict[str, int]</code>)Locations (Pydantic model) <p>We can generate a variety of names by providing instructions. Note the default behavior is to generate a list of strings:</p> <pre><code>import marvin\n\nnames = marvin.generate(\n    n=4, instructions=\"first names\"\n)\n\nfrench_names = marvin.generate(\n    n=4, instructions=\"first names from France\"\n)\n\nstar_wars_names = marvin.generate(\n    n=4, instructions=\"first names from Star Wars\"\n)\n</code></pre> <p>Result</p> <pre><code>names == ['John', 'Emma', 'Michael', 'Sophia']\n\nfrench_names == ['Jean', 'Claire', 'Lucas', 'Emma']\n\nstar_wars_names == ['Luke', 'Leia', 'Han', 'Anakin']\n</code></pre> <p>By providing a target type, we can generate dictionaries that map countries to their populations:</p> <pre><code>from pydantic import BaseModel\n\npopulations = marvin.generate(\n    target=dict[str, int],\n    n=4, \n    instructions=\"a map of country: population\",\n)\n</code></pre> <p>Result</p> <pre><code>populations == [\n    {'China': 1444216107},\n    {'India': 1380004385},\n    {'United States': 331893745},\n    {'Indonesia': 276361783},\n]\n</code></pre> <p>Pydantic models can also be used as targets. Here's a list of US cities named for presidents:</p> <pre><code>from pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.generate(\n    target=Location, \n    n=4, \n    instructions=\"cities in the United States named after presidents\"\n)\n</code></pre> <p>Result</p> <pre><code>locations == [\n    Location(city='Washington', state='District of Columbia'),\n    Location(city='Jackson', state='Mississippi'),\n    Location(city='Cleveland', state='Ohio'),\n    Location(city='Lincoln', state='Nebraska'),\n]\n</code></pre> <p>How it works</p> <p>     Marvin instructs the LLM to generate a list of JSON objects that satisfy the provided schema and instructions. Care is taken to introduce variation in the output, so that the samples are not all identical.   </p>"},{"location":"docs/text/generation/#generating-data","title":"Generating data","text":"<p>The <code>generate</code> function is the primary tool for generating synthetic data. It accepts a <code>type</code> argument, which can be any Python type, Pydantic model, or <code>Literal</code>. It also has an argument <code>n</code>, which specifies the number of samples to generate. Finally, it accepts an <code>instructions</code> argument, which is a natural language description of the desired output. The LLM will use these instructions, in addition to the provided type, to guide its generation process. Instructions are especially important for types that are not self documenting, such as Python builtins like <code>str</code> and <code>int</code>.</p>"},{"location":"docs/text/generation/#supported-targets","title":"Supported targets","text":"<p><code>generate</code> supports almost all builtin Python types, plus Pydantic models, Python's <code>Literal</code>, and <code>TypedDict</code>. Pydantic models are especially useful for specifying specific features of the generated data, such as locations, dates, or more complex types. Builtin types are most useful in conjunction with instructions that provide more precise criteria for generation.</p> <p>To specify the output type, pass it as the <code>target</code> argument to <code>generate</code>. The function will always return a list of <code>n</code> items of the specified type. If no target is provided, <code>generate</code> will return a list of strings.</p> <p>Avoid tuples</p> <p>OpenAI models currently have trouble parsing the API representation of tuples. Therefore we recommend using lists or Pydantic models (for more strict typing) instead. Tuple support will be added in a future release.</p>"},{"location":"docs/text/generation/#instructions","title":"Instructions","text":"<p>Data generation relies even more on instructions than other Marvin tools, as the potential for variation is much greater. Therefore, you should provide as much detail as possible in your instructions, in addition to any implicit documentation in your requested type. </p> <p>Instructions are freeform natural language and can be as general or specific as you like. The LLM will do its best to comply with any instructions you give.</p>"},{"location":"docs/text/generation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>generate</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/generation/#caching","title":"Caching","text":"<p>Normally, each <code>generate</code> call would be independent. For some prompts, this would mean that each call produced very similar results to other calls. That would mean that generating, say, 10 items in a single call would produce a much more varied and high-quality result than generating 10 items in 5 calls of 2 items each.</p> <p>To mediate this issue, Marvin maintains an in-memory cache of the last 100 results produced by each <code>generate</code> prompt. These responses are shown to the LLM during generation to encourage variation. Note that the cache is not persisted across Python sessions. Cached results are also subject to a token cap to avoid flooding the LLM's context window. The token cap can be set with <code>MARVIN_AI_TEXT_GENERATE_CACHE_TOKEN_CAP</code> and defaults to 600.</p> <p>To disable this behavior, pass <code>use_cache=False</code> to <code>generate</code>.</p> <p>Here is an example of how the cache improves generation. The first tab shows 10 cities generated in a single call; the second shows 10 cities generated in 5 calls of 2 cities each; and the third shows 10 cities generated in 5 calls but with the cache disabled.</p> <p>The first and second tabs both show high-quality, varied results. The third tab is more disappointing, as it shows almost no variation.</p> Single callFive calls, with cachingFive calls, without caching <p>Generate 10 cities in a single call, which produces a varied list:</p> <pre><code>cities = marvin.generate(n=10, instructions='major US cities')\n</code></pre> <p>Result</p> <pre><code>cities == [\n    'New York',\n    'Los Angeles',\n    'Chicago',\n    'Houston',\n    'Phoenix',\n    'Philadelphia',\n    'San Antonio',\n    'San Diego',\n    'Dallas',\n    'San Jose'\n]\n</code></pre> <p>Generate 10 cities in a five calls, using the cache. This also produces a varied list: <pre><code>cities = []\nfor _ in range(5):\n    cities.extend(marvin.generate(n=2, instructions='major US cities'))\n</code></pre></p> <p>Result</p> <pre><code>cities == [\n    'Chicago',\n    'San Francisco',\n    'Seattle',\n    'New York City',\n    'Los Angeles',\n    'Houston',\n    'Miami',\n    'Dallas',\n    'Atlanta',\n    'Boston'\n]\n</code></pre> <p>Generate 10 cities in five calls, without the cache. This produces a list with almost no variation, since each call is independent:</p> <pre><code>cities = []\nfor _ in range(5):\n    cities.extend(marvin.generate(\n        n=2, \n        instructions='major US cities', \n        use_cache=False,\n))\n</code></pre> <p>Result</p> <pre><code>cities == [\n    'Houston',\n    'Seattle',\n    'Chicago',\n    'Houston',\n    'Chicago',\n    'Houston',\n    'Chicago',\n    'Houston',\n    'Los Angeles',\n    'Houston'\n]\n</code></pre>"},{"location":"docs/text/generation/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>generate_async</code>:</p> <pre><code>cat_names = await marvin.generate_async(\n    n=4, \n    instructions=\"names for cats inspired by Chance the Rapper\"\n)\n\n# ['Chancey', 'Rappurr', 'Lyric', 'Chano']\n</code></pre>"},{"location":"docs/text/transformation/","title":"Converting text to data","text":"<p>At the heart of Marvin is the ability to convert natural language to native Python types and structured objects. This is one of its simplest but most powerful features, and forms the basis for almost every other tool. </p> <p>The primary tool for creating structured data is the <code>cast</code> function, which takes a natural language string as its input, as well as a type to which the text should be converted.</p> <p>What it does</p> <p>     The <code>cast</code> function transforms natural language text into a Python type or structured object.   </p> <p>Example</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nmarvin.cast(\"the big apple\", target=Location)\n</code></pre> <p>Result</p> <pre><code>Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>How it works</p> <p>     Marvin creates a schema from the provided type and instructs the LLM to use the schema to format its JSON response.   </p> <p>     In Python, the JSON representation is hydrated into a \"full\" instance of the type.   </p>"},{"location":"docs/text/transformation/#supported-types","title":"Supported types","text":"<p>The <code>cast</code> function supports conversion almost all builtin Python types, plus Pydantic models and Python's <code>Literal</code>, and <code>TypedDict</code>. When called, the LLM will take all available information into account, performing deductive reasoning if necessary, to determine the best output. The result will be a Python object of the provided type.</p>"},{"location":"docs/text/transformation/#instructions","title":"Instructions","text":"<p>Sometimes the cast operation is obvious, as in the \"big apple\" example above. Other times, it may be more nuanced. In these cases, the LLM may require guidance or examples to make the right decision. You can provide natural language <code>instructions</code> when calling <code>cast()</code> in order to steer the output. </p> <p>In a simple case, instructions can be used independent of any type-casting. Here, we want to keep the output a string, but get the 2-letter abbreviation of the state.</p> <pre><code>marvin.cast('California', target=str, instructions=\"The state's abbreviation\")\n# \"CA\"\n\nmarvin.cast('The sunshine state', target=str, instructions=\"The state's abbreviation\")\n# \"FL\"\n\nmarvin.cast('Mass.', target=str, instructions=\"The state's abbreviation\")\n# MA\n</code></pre>"},{"location":"docs/text/transformation/#classification","title":"Classification","text":"<p>One way of classifying text is by casting it to a constrained type, such as an <code>Enum</code> or <code>bool</code>. This forces the LLM to choose one of the provided options.</p> <p>Marvin provides a dedicated <code>classify</code> function for this purpose. As a convenience, <code>cast</code> will automatically switch to <code>classify</code> when given a constrained target type. However, you may prefer to use the <code>classify</code> function to make your intent more clear to other developers.</p>"},{"location":"docs/text/transformation/#ai-models","title":"AI models","text":"<p>In addition to providing Pydantic models as <code>cast</code> targets, Marvin has a drop-in replacement for Pydantic's <code>BaseModel</code> that permits instantiating the model with natural language. These \"AI Models\" can be created in two different ways:</p> <ol> <li>Decorating a BaseModel with <code>@marvin.model</code>.</li> <li>Subclassing the <code>marvin.Model</code> class</li> </ol> <p>Though these are roughly equivalent, we recommend the decorator as it will make the intent more clear to other developers (in particular, it will not hide that the model is a <code>BaseModel</code>).</p> <p>Here is the class decorator:</p> <pre><code>import marvin\n\n\n@marvin.model\nclass Location:\n    city: str\n    state: str\n\n\nLocation('CHI')\n# Location(city=\"Chicago\", state=\"IL\")\n</code></pre> <p>And here is the equivalent subclass:</p> <pre><code>import marvin\n\n\nclass Location(marvin.Model):\n    city: str\n    state: str\n\n\nLocation('CHI')\n# Location(city=\"Chicago\", state=\"IL\")\n</code></pre>"},{"location":"docs/text/transformation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>cast</code> or <code>@model</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/transformation/#async-support","title":"Async support","text":"<p>If you are using <code>marvin</code> in an async environment, you can use <code>cast_async</code>:</p> <pre><code>result = await marvin.cast_async(\"one\", int) \n\nassert result == 1\n</code></pre>"},{"location":"docs/vision/captioning/","title":"Captioning images","text":"<p>Marvin can use OpenAI's vision API to process images as inputs. </p> <p>Beta</p> <p>Please note that vision support in Marvin is still in beta, as OpenAI has not finalized the vision API yet. While it works as expected, it is subject to change.</p> <p>What it does</p> <p>     The <code>caption</code> function generates text from images.   </p> <p>Example</p> <p>Generate a description of the following image, hypothetically available at <code>/path/to/marvin.png</code>:</p> <p></p> <pre><code>import marvin\nfrom pathlib import Path\n\ncaption = marvin.beta.caption(image=Path('/path/to/marvin.png'))\n</code></pre> <p>Result</p> <p>\"This is a digital illustration featuring a stylized, cute character resembling a Funko Pop vinyl figure with large, shiny eyes and a square-shaped head, sitting on abstract wavy shapes that simulate a landscape. The whimsical figure is set against a dark background with sparkling, colorful bokeh effects, giving it a magical, dreamy atmosphere.\"</p> <p>How it works</p> <p>     Marvin passes your images to the OpenAI vision API as part of a larger prompt.   </p>"},{"location":"docs/vision/captioning/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>caption</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/vision/captioning/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>caption_async</code>:</p> <pre><code>caption = await marvin.beta.caption_async(image=Path('/path/to/marvin.png'))\n</code></pre>"},{"location":"docs/vision/classification/","title":"Classifying images","text":"<p>Marvin can use OpenAI's vision API to process images and classify them into categories.</p> <p>The <code>marvin.beta.classify</code> function is an enhanced version of <code>marvin.classify</code> that accepts images as well as text. </p> <p>Beta</p> <p>Please note that vision support in Marvin is still in beta, as OpenAI has not finalized the vision API yet. While it works as expected, it is subject to change.</p> <p>What it does</p> <p>     The <code>classify</code> function can classify images as one of many labels.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual classify operation is performed with an LLM.    </p> <p>Example</p> <p>We will classify the animal in this image, as well as whether it is wet or dry:</p> <p></p> <pre><code>import marvin\n\nimg = marvin.beta.Image('https://upload.wikimedia.org/wikipedia/commons/d/d5/Retriever_in_water.jpg')\n\nanimal = marvin.beta.classify(\n    img, \n    labels=['dog', 'cat', 'bird', 'fish', 'deer']\n)\n\ndry_or_wet = marvin.beta.classify(\n    img, \n    labels=['dry', 'wet'], \n    instructions='Is the animal wet?'\n)\n</code></pre> <p>Result</p> <pre><code>assert animal == 'dog'\nassert dry_or_wet == 'wet'\n</code></pre>"},{"location":"docs/vision/classification/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> and <code>vision_model_kwargs</code> arguments of <code>classify</code>. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/vision/classification/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>classify_async</code>:</p> <pre><code>result = await marvin.beta.classify_async(\n    \"The app crashes when I try to upload a file.\", \n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n) \n\nassert result == \"bug\"\n</code></pre>"},{"location":"docs/vision/extraction/","title":"Extracting entities from images","text":"<p>Marvin can use OpenAI's vision API to process images and convert them into structured data, transforming unstructured information into native types that are appropriate for a variety of programmatic use cases.</p> <p>The <code>marvin.beta.extract</code> function is an enhanced version of <code>marvin.extract</code> that accepts images as well as text.</p> <p>Beta</p> <p>Please note that vision support in Marvin is still in beta, as OpenAI has not finalized the vision API yet. While it works as expected, it is subject to change.</p> <p>What it does</p> <p>     The beta <code>extract</code> function can extract entities from images and text.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual extract operation is performed with an LLM.    </p> <p>Example: identifying dogs</p> <p>We will extract the breed of each dog in this image:</p> <p></p> <pre><code>import marvin\n\nimg = marvin.beta.Image(\n    \"https://images.unsplash.com/photo-1548199973-03cce0bbc87b?\",\n)\n\nresult = marvin.beta.extract(img, target=str, instructions=\"dog breeds\")\n</code></pre> <p>Result</p> <pre><code>result == [\"Pembroke Welsh Corgi\", \"Yorkshire Terrier\"]\n</code></pre>"},{"location":"docs/vision/extraction/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> and <code>vision_model_kwargs</code> arguments of <code>extract</code>. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/vision/extraction/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>extract_async</code>:</p> <pre><code>result = await marvin.beta.extract_async(\n    \"I drove from New York to California.\",\n    target=str,\n    instructions=\"2-letter state codes\",\n) \n\nassert result == [\"NY\", \"CA\"]\n</code></pre>"},{"location":"docs/vision/transformation/","title":"Converting images to data","text":"<p>Marvin can use OpenAI's vision API to process images and convert them into structured data, transforming unstructured information into native types that are appropriate for a variety of programmatic use cases.</p> <p>The <code>marvin.beta.cast</code> function is an enhanced version of <code>marvin.cast</code> that accepts images as well as text.</p> <p>Beta</p> <p>Please note that vision support in Marvin is still in beta, as OpenAI has not finalized the vision API yet. While it works as expected, it is subject to change.</p> <p>What it does</p> <p>     The <code>cast</code> function can cast images to structured types.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual cast operation is performed with an LLM.    </p> <p>Example: locations</p> <p>We will cast this image to a <code>Location</code> type:</p> <p></p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description=\"2-letter state abbreviation\")\n\n\nimg = marvin.beta.Image(\n    \"https://images.unsplash.com/photo-1568515387631-8b650bbcdb90\",\n)\nresult = marvin.beta.cast(img, target=Location)\n</code></pre> <p>Result</p> <pre><code>assert result == Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>Example: getting information about a book</p> <p>We will cast this image to a <code>Book</code> to extract key information:</p> <p></p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    subtitle: str\n    authors: list[str]\n\n\nimg = marvin.beta.Image(\n    \"https://hastie.su.domains/ElemStatLearn/CoverII_small.jpg\",\n)\nresult = marvin.beta.cast(img, target=Book)\n</code></pre> <p>Result</p> <pre><code>assert result == Book(\n    title='The Elements of Statistical Learning',\n    subtitle='Data Mining, Inference, and Prediction',\n    authors=['Trevor Hastie', 'Robert Tibshirani', 'Jerome Friedman']\n)\n</code></pre>"},{"location":"docs/vision/transformation/#instructions","title":"Instructions","text":"<p>If the target type isn't self-documenting, or you want to provide additional guidance, you can provide natural language <code>instructions</code> when calling <code>cast</code> in order to steer the output. </p> <p>Example: checking groceries</p> <p>Let's use this image to see if we got everything on our shopping list:</p> <p></p> <pre><code>import marvin\n\nshopping_list = [\"bagels\", \"cabbage\", \"eggs\", \"apples\", \"oranges\"]\n\nmissing_items = marvin.beta.cast(\n    marvin.beta.Image(\"https://images.unsplash.com/photo-1588964895597-cfccd6e2dbf9\"), \n    target=list[str], \n    instructions=f\"Did I forget anything on my list: {shopping_list}?\",\n)\n</code></pre> <p>Result</p> <pre><code>assert missing_items == [\"eggs\", \"oranges\"]\n</code></pre>"},{"location":"docs/vision/transformation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> and <code>vision_model_kwargs</code> arguments of <code>cast</code>. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/vision/transformation/#async-support","title":"Async support","text":"<p>If you are using <code>marvin</code> in an async environment, you can use <code>cast_async</code>:</p> <pre><code>result = await marvin.beta.cast_async(\"one\", int) \n\nassert result == 1\n</code></pre>"},{"location":"examples/being_specific_about_types/","title":"Fully leveraging <code>pydantic</code>","text":""},{"location":"examples/being_specific_about_types/#annotated-and-field","title":"<code>Annotated</code> and <code>Field</code>","text":"<p>Numbers in a valid range</p> <p>Pydantic's <code>Field</code> lets us be very specific about what we want from the LLM.</p> <pre><code>from typing import Annotated\nimport marvin\nfrom pydantic import Field\nfrom typing_extensions import TypedDict\n\nActivationField = Field(\n    description=(\n        \"A score between -1 (not descriptive) and 1\"\n        \" (very descriptive) for the given emotion\"\n    ),\n    ge=-1,\n    le=1\n)\n\nSentimentActivation = Annotated[float, ActivationField]\n\nclass DetailedSentiment(TypedDict):\n    happy: SentimentActivation\n    sad: SentimentActivation\n    angry: SentimentActivation\n    surprised: SentimentActivation\n    amused: SentimentActivation\n    scared: SentimentActivation\n\n@marvin.fn\ndef sentiment_analysis(text: str) -&gt; DetailedSentiment:\n    \"\"\"Analyze the sentiment of a given text\"\"\"\n\nsentiment_analysis(\n    \"dude i cannot believe how hard that\"\n    \" kangaroo just punched that guy \ud83e\udd23\"\n    \" - he really had it coming, but glad he's ok\"\n)\n</code></pre> <p>Result</p> <pre><code>{\n    'happy': 0.8,\n    'sad': -0.1,\n    'angry': -0.2,\n    'surprised': 0.7,\n    'amused': 1.0,\n    'scared': -0.1\n}\n</code></pre>"},{"location":"examples/being_specific_about_types/#complex-types","title":"Complex types","text":"<p>Using <code>BaseModel</code> and <code>Field</code></p> <p>To parse and validate complex nested types, use <code>BaseModel</code> and <code>Field</code>:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\nclass Location(BaseModel):\n    city: str\n    state: str | None = Field(description=\"Two-letter state code\")\n    country: str\n    latitute: float | None = Field(\n        description=\"Latitude in degrees\",\n        ge=-90,\n        le=90\n    )\n    longitude: float | None = Field(\n        description=\"Longitude in degrees\",\n        ge=-180,\n        le=180\n    )\n\nclass Traveler(BaseModel):\n    name: str\n    age: int | None = Field(description=\"Age in years\")\n\nclass Trip(BaseModel):\n    travelers: list[Traveler]\n    origin: Location\n    destination: Location\n\ntrip = marvin.model(Trip)(\n    \"Marvin and Ford are heading from Chi to SF for their 30th birthdays\"\n)\n</code></pre> <p>Result</p> <pre><code>Trip(\n    travelers=[\n        Traveler(name='Marvin', age=30),\n        Traveler(name='Ford', age=30)\n    ],\n    origin=Location(\n        city='Chicago',\n        state='IL',\n        country='USA',\n        latitute=41.8781,\n        longitude=-87.6298\n    ),\n    destination=Location(\n        city='San Francisco',\n        state='CA',\n        country='USA',\n        latitute=37.7749,\n        longitude=-122.4194\n    )\n)\n</code></pre>"},{"location":"examples/call_routing/","title":"Customer call routing","text":"<p>Automatically route customer calls to the right department.</p> <p>Call routing</p> <pre><code>import marvin\nfrom enum import Enum\n\n\nclass Department(Enum):\n    SALES = \"sales\"\n    SUPPORT = \"support\"\n    BILLING = \"billing\"\n\n\n# define a convenience function\ndef route_call(transcript: str) -&gt; Department:\n    return marvin.classify(\n        transcript,\n        labels=Department,\n        instructions=\"Select the best department for the customer request\",\n    )\n</code></pre> <p>\ud83d\udcb3 Update payment method</p> <pre><code>department = route_call(\"I need to update my payment method\")\nassert department == Department.BILLING\n</code></pre> <p>\ud83d\udcb5 Price matching</p> <pre><code>department = route_call(\"Well FooCo offered me a better deal\")\nassert department == Department.SALES\n</code></pre> <p>\ud83e\udd2c Angry noises</p> <pre><code>department = route_call(\"*angry noises*\")\nassert department == Department.SUPPORT\n</code></pre>"},{"location":"examples/deduplication/","title":"Entity Deduplication","text":"How many distinct cities are there in the following text? <pre><code>windy city from illnois, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco,\n</code></pre> <p>We can look and see the answer is 3, but how can we arrive here programmatically?</p> <p>In this section, we'll explore using <code>marvin</code> to extract entities so we can use them directly in normal Python code.</p>"},{"location":"examples/deduplication/#creating-our-entity","title":"Creating our entity","text":"<p>To extract and deduplicate entities, we'll need to create an entity, i.e. the thing we are looking for.</p> <p>In this case, we're looking for cities, so we'll create a <code>City</code> entity.</p> <pre><code>from pydantic import BaseModel\n\nclass City(BaseModel):\n    informal_name: str\n    standard_name: str\n    state: str | None = None\n    country: str | None = None\n</code></pre> <p>We want to keep its raw name (<code>informal_name</code>) and its official name (<code>standard_name</code>), as well as its state and country, if applicable.</p>"},{"location":"examples/deduplication/#extracting-entities","title":"Extracting entities","text":"<p>Now we can use <code>marvin.extract</code> to get a <code>list[City]</code> from our text.</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass City(BaseModel):\n    informal_name: str\n    standard_name: str\n    state: str | None = None\n    country: str | None = None\n\ncities = marvin.extract(\n    \"windy city from illnois, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco\",\n    City,\n    instructions=\"Be sure to identify the origin country of the city if possible.\",\n)\n\nprint(\n    set(total_entities := [city.standard_name for city in cities]),\n    f\" | {len(total_entities)=}\"\n)\n\nprint(\n    \"\\n\"+\"\\n\".join(\n        city.model_dump_json(indent=2)\n        for city in cities\n    )\n)\n</code></pre> Click for the output <pre><code>{'San Francisco', 'New York', 'Chicago'}  | len(total_entities)=7\n\n{\n    \"informal_name\": \"Windy City\",\n    \"standard_name\": \"Chicago\",\n    \"state\": \"Illinois\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"The Windy City\",\n    \"standard_name\": \"Chicago\",\n    \"state\": \"Illinois\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"New York City\",\n    \"standard_name\": \"New York\",\n    \"state\": \"New York\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"The Big Apple\",\n    \"standard_name\": \"New York\",\n    \"state\": \"New York\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"SF\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"San Fran\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"San Francisco\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n</code></pre>"},{"location":"examples/python_augmented_prompts/","title":"Augmenting prompts with Python","text":""},{"location":"examples/python_augmented_prompts/#web-scraping","title":"Web scraping","text":"<p>Fetch rich prompt material with Python</p> <p>Using an http client to fetch HTML that an LLM will filter for a <code>list[RelatedArticle]</code>:</p> <pre><code>import bs4\nimport httpx\nimport marvin\nfrom typing_extensions import TypedDict\n\nclass RelatedArticle(TypedDict):\n    title: str\n    link: str\n\n\n@marvin.fn\ndef retrieve_HN_articles(topic: str | None = None) -&gt; list[RelatedArticle]:\n    \"\"\"Retrieve only articles from HN that are related to a given topic\"\"\"\n    response = httpx.get(\"https://news.ycombinator.com/\")\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    return [\n        (link.text, link['href']) for link in soup.select('.titleline a')\n    ]\n\nretrieve_HN_articles(\"rust\")\n</code></pre> <p>Result</p> <pre><code>[\n    {\n        'title': 'A lowering strategy for control effects in Rust',\n        'link': 'https://www.abubalay.com/blog/2024/01/14/rust-effect-lowering'\n    },\n    {\n        'title': 'Show HN: A minimal working Rust / SDL2 / WASM browser game',\n        'link': 'https://github.com/awwsmm/hello-rust-sdl2-wasm'\n    }\n]\n</code></pre> <p>Note</p> <p>You could also use <code>marvin.extract</code> to extract the <code>list[RelatedArticle]</code> from the output of the un-decorated function <code>retrieve_HN_articles</code>:</p> <pre><code>related_articles = marvin.extract(retrieve_HN_articles(), RelatedArticle)\n</code></pre>"},{"location":"examples/python_augmented_prompts/#vectorstore-based-rag","title":"Vectorstore-based RAG","text":"<p>Stuff <code>top k</code> document excerpts into a prompt</p> <p>Using an http client to fetch HTML that an LLM will filter for a <code>list[RelatedArticle]</code>:</p> <pre><code>from typing_extensions import TypedDict\nimport marvin\nfrom marvin.tools.chroma import query_chroma # you must have a vectorstore with embedded documents\nfrom marvin.utilities.asyncio import run_sync\n\nclass Answer(TypedDict):\n    answer: str\n    supporting_links: list[str] | None\n\n@marvin.fn\ndef answer_question(\n    question: str,\n    top_k: int = 2,\n    style: str = \"concise\"\n) -&gt; Answer:\n    \"\"\"Answer a question given supporting context in the requested style\"\"\"\n    return run_sync(query_chroma(question, n_results=top_k))\n\nanswer_question(\"What are prefect blocks?\", style=\"pirate\")\n</code></pre> <p>Result</p> <pre><code>{\n    'answer': \"Ahoy! Prefect blocks be a primitive within Prefect fer storin' configuration and interfacin' with th' external systems. Ye can use 'em to manage credentials and interact with services like AWS, GitHub, and Slack. Arr, they be comin' with methods for uploadin' or downloadin' data, among other actions, and ye can register new ones with Prefect Cloud or server.\",\n    'supporting_links': ['https://docs.prefect.io/latest/concepts/blocks/']\n}\n</code></pre>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>~/.marvin/.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-fastapi-app-to-handle-slack-events","title":"Define a FastAPI app to handle Slack events","text":"<p><pre><code>@app.post(\"/chat\")\nasync def chat_endpoint(request: Request):\n    payload = SlackPayload(**await request.json())\n    match payload.type:\n        case \"event_callback\":\n            asyncio.create_task(handle_message(payload))\n        case \"url_verification\":\n            return {\"challenge\": payload.challenge}\n        case _:\n            raise HTTPException(400, \"Invalid event type\")\n\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=4200)\n</code></pre> Here, we define a simple FastAPI endpoint / app to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#handle-generating-the-ai-response","title":"Handle generating the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def handle_message(payload: dict) -&gt; str:\n    # somehow generate the ai responses\n    ...\n\n    # post the response to slack\n    _post_message(\n        messsage=some_message_ive_constructed,\n        channel=event.get(\"channel\", \"\"),\n        thread_ts=thread_ts,\n    )\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, etc</li> </ul>"},{"location":"examples/slackbot/#example-implementation-of-handler-prefect-community-slackbot","title":"Example implementation of handler: Prefect Community Slackbot","text":"<p>This runs 24/7 in the #ask-marvin channel of the Prefect Community Slack. It responds to users in a thread, and has memory of previous messages by slack thread. It uses the <code>chroma</code> and <code>github</code> tools for RAG to answer questions about Prefect 2.x.</p> <pre><code>async def handle_message(payload: SlackPayload): # SlackPayload is a pydantic model \n    logger = get_logger(\"slackbot\")\n    user_message = (event := payload.event).text\n    cleaned_message = re.sub(BOT_MENTION, \"\", user_message).strip()\n    logger.debug_kv(\"Handling slack message\", user_message, \"green\")\n    if (user := re.search(BOT_MENTION, user_message)) and user.group(\n        1\n    ) == payload.authorizations[0].user_id:\n        thread = event.thread_ts or event.ts\n        assistant_thread = CACHE.get(thread, Thread())\n        CACHE[thread] = assistant_thread\n\n        await handle_keywords.submit(\n            message=cleaned_message,\n            channel_name=await get_channel_name(event.channel),\n            asking_user=event.user,\n            link=(  # to user's message\n                f\"{(await get_workspace_info()).get('url')}archives/\"\n                f\"{event.channel}/p{event.ts.replace('.', '')}\"\n            ),\n        )\n\n        with Assistant(\n            name=\"Marvin (from Hitchhiker's Guide to the Galaxy)\",\n            tools=[task(multi_query_chroma), task(search_github_issues)],\n            instructions=(\n                \"use chroma to search docs and github to search\"\n                \" issues and answer questions about prefect 2.x.\"\n                \" you must use your tools in all cases except where\"\n                \" the user simply wants to converse with you.\"\n            ),\n        ) as assistant:\n            user_thread_message = await assistant_thread.add_async(cleaned_message)\n            await assistant_thread.run_async(assistant)\n            ai_messages = assistant_thread.get_messages(\n                after_message=user_thread_message.id\n            )\n            await task(post_slack_message)(\n                ai_response_text := \"\\n\\n\".join(\n                    m.content[0].text.value for m in ai_messages\n                ),\n                channel := event.channel,\n                thread,\n            )\n            logger.debug_kv(\n                success_msg := f\"Responded in {channel}/{thread}\",\n                ai_response_text,\n                \"green\",\n            )\n</code></pre> <p>This is just an example</p> <p>There are many ways to implement a Slackbot with Marvin's Assistant SDK / utils, FastAPI is just our favorite.</p> <p>Run this file with something like: <pre><code>python start.py\n</code></pre></p> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y git build-essential &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN pip install \".[slackbot]\"\n\nEXPOSE 4200\n\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here","text":"<ul> <li>cookbook/slackbot/start.py</li> </ul>"},{"location":"examples/xkcd_bird/","title":"xkcd bird classifier","text":"<p>Is this a bird?</p> <p> <pre><code>import marvin\n\nphoto = marvin.beta.Image(\n    \"https://images.unsplash.com/photo-1613891188927-14c2774fb8d7\",\n)\n\nresult = marvin.beta.classify(\n    photo,\n    labels=[\"bird\", \"not bird\"]\n)\n</code></pre></p> <p>Yes!</p> <pre><code>assert result == \"bird\"\n</code></pre>"},{"location":"examples/hogwarts_sorting_hat/hogwarts_sorting_hat/","title":"Hogwarts sorting hat","text":"<p>Hogwarts sorting hat</p> <pre><code>import marvin\n\nstudent = \"Brave, daring, chivalrous, and sometimes a bit reckless.\"\n\nhouse = marvin.classify(\n    student,\n    labels=[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n)\n</code></pre> <p>Welcome to Gryffindor!</p> <pre><code>assert house == \"Gryffindor\"\n</code></pre>"},{"location":"examples/michael_scott_business/michael_scott_business/","title":"Michael Scott's four kinds of businesses","text":"<p>What kind of business are LLMs?</p> <pre><code>import marvin\n\nbusinesses = [\n    \"tourism\",\n    \"food service\",\n    \"railroads\",\n    \"sales\",\n    \"hospitals/manufacturing\",\n    \"air travel\",\n]\n\nresult = marvin.classify(\"LLMs\", labels=businesses)\n</code></pre> <p>Tourism</p> <pre><code>assert result == \"tourism\"\n</code></pre>"},{"location":"help/legacy_docs/","title":"Legacy Documentation","text":"<p>If you want to view documentation of previous versions of Marvin, here's a step by step guide on how to do that.</p>"},{"location":"help/legacy_docs/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>Git</code></li> <li><code>python3</code></li> <li><code>pip</code></li> </ul>"},{"location":"help/legacy_docs/#automated-script-for-legacy-documentation","title":"Automated Script for Legacy Documentation","text":"<p>To build and view the docs for a specific version of Marvin, you can use this script.</p> <p>You can either clone the Marvin repo and run the script locally, or copy the script and run it directly in your terminal after making it executable: <pre><code># unix\nchmod +x scripts/serve_legacy_docs\n\n# run the script (default version is v1.5.6)\n./scripts/serve_legacy_docs\n\n# optionally, specify a version\n./scripts/serve_legacy_docs v1.5.3\n</code></pre></p>"},{"location":"help/legacy_docs/#manual-steps","title":"Manual Steps","text":"<p>If you prefer to manually perform the steps or need to tailor them for your specific operating system, follow these instructions:</p> <ol> <li> <p>Clone the Repository    Clone the Marvin repository using Git:    <pre><code>git clone https://github.com/PrefectHQ/marvin.git\ncd marvin\n</code></pre></p> </li> <li> <p>Checkout the Specific Tag    Checkout the tag for the version you are interested in. Replace <code>v1.5.6</code> with the desired version tag:    <pre><code>git fetch --tags\ngit checkout tags/v1.5.6\n</code></pre></p> </li> <li> <p>Create a Virtual Environment    Create and activate a virtual environment to isolate the dependency installation:    <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install Dependencies    Install the necessary dependencies for the documentation:    <pre><code>pip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Serve the Documentation Locally    Use <code>mkdocs</code> to serve the documentation:    <pre><code>mkdocs serve\n</code></pre>    This will start a local server. View the documentation by navigating to <code>http://localhost:8000</code> in your web browser.</p> </li> <li> <p>Exit Virtual Environment    Once finished, you can exit the virtual environment:    <pre><code>deactivate\n</code></pre></p> </li> </ol> <p>Optionally, you can remove the virtual environment folder:    <pre><code>rm -rf venv\n</code></pre></p>"},{"location":"utilities%20%28old%29/chat_completion/","title":"Chat completion","text":"<p>In Marvin, each supported Large Language Model can be accessed with one common API. This means that  you can easily switch between providers without having to change your code. We have anchored our API  to mirror that of OpenAI's Python SDK. </p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"utilities%20%28old%29/chat_completion/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\n\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\n\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n\n# Call the instance with create. \nopenai_pirate.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\n\n\nopenai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre>"},{"location":"utilities%20%28old%29/chat_completion/#advanced-use","title":"Advanced Use","text":""},{"location":"utilities%20%28old%29/chat_completion/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass CoffeeOrder(BaseModel):\n    size: Literal['small', 'medium', 'large']\n    milk: Literal['soy', 'oat', 'dairy']\n    with_sugar: bool = False\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Translation(BaseModel):\n    spanish: str\n    french: str\n    swedish: str\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [\n    {\n        'role': 'system',\n        'content': 'You translate user messages into other languages.'\n    },\n    {\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"utilities%20%28old%29/chat_completion/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put it $100 every month for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n    \"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\n    A = P * (1 + r/n)**(n*t)\n    return round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put in $50/mo for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"utilities%20%28old%29/chat_completion/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\n\ndef divide(x: float, y: float) -&gt; str:\n    '''Divides x and y'''\n    return str(x/y)\n\ndef add(x: int, y: int) -&gt; str:\n    '''Adds x and y'''\n    return str(x+y)\n\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n\n    # Start off with an external question / prompt. \n    prompt = 'What is 4124124 + 424242 divided by 48124?'\n\n    # Initialize the conversation with a prompt from the user. \n    conversation.send(messages = [{'role': 'user', 'content': prompt}])\n\n    # While the most recent turn has a function call, evaluate it. \n    while conversation.last_response.has_function_call():\n\n        # Send the most recent function call to the conversation. \n        conversation.send(messages = [\n            conversation.last_response.call_function() \n        ])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installation","text":"<p>Install Marvin with <code>pip</code>:</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin version</code> in your terminal. </p> <p>Upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre>"},{"location":"welcome/installation/#tutorial","title":"Tutorial","text":"<p>Now that you've installed Marvin, check out the tutorial to learn how to use it.</p>"},{"location":"welcome/installation/#requirements","title":"Requirements","text":"<p>Marvin requires Python 3.9 or greater, and is tested on all major Python versions and operating systems.</p>"},{"location":"welcome/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"welcome/overview/","title":"Marvin Documentation","text":"<p>Marvin is a Python library that lets you use Large Language Models by writing code, not prompts. It's open source, free to use, rigorously type-hinted, used by thousands of engineers, and built by the engineering team at Prefect.</p> <p>Marvin is lightweight and is built for incremental adoption. You can use it purely as a serialization library and bring your own stack, or fully use its engine to work with OpenAI and other providers. </p> How Marvin feels Structured Data ExtractionText ClassificationBusiness Logic <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n    latitude: float\n    longitude: float\n\nmarvin.model(Location)(\"They say they're from the Windy City!\")\n# Location(city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298)\n</code></pre> Notice there's no code written, just the expected types. Marvin's components turn your function into a prompt, uses AI to get its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import classifier\nfrom typing import Literal\n\n@classifier\ndef customer_intent(text: str) -&gt; Literal['Store Hours', 'Pharmacy', 'Returns']:\n    \"\"\"Classifies incoming customer intent\"\"\"\n\ncustomer_intent(\"I need to pick up my prescription\") # \"Pharmacy\"\n</code></pre> Notice <code>customer_intent</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>import marvin\n\n@marvin.fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"Generates a list of {{ n }} {{ color }} fruits\"\"\"\n\nlist_fruits(3) # \"['Apple', 'Cherry', 'Strawberry']\"\n</code></pre> Notice <code>list_fruits</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Learning Marvin</p> <p>If you know Python, you already know Marvin. There are no fancy abstractions, just a handful of low-level, customizable decorators  to give your existing code superpowers and a number of utilities that make your life as an AI Engineer easier no matter what framework you use. </p> Sections Description Configuration Details on setting up Marvin and configuring various aspects of its behavior AI Components Documentation for Marvin's familiar, Pythonic interfaces to AI-powered functionality. API Utilities Low level API for building prompts and calling LLMs Examples Deeper dives into how to use Marvin"},{"location":"welcome/tutorial/","title":"Tutorial","text":""},{"location":"welcome/tutorial/#installing-marvin","title":"Installing Marvin","text":"<p>Before we can start, you'll need to install Marvin. Come back here when you're done!</p> <p>(Spoiler alert: run <code>pip install marvin -U</code>.)</p>"},{"location":"welcome/tutorial/#getting-an-openai-api-key","title":"Getting an OpenAI API Key","text":"<p>Marvin uses OpenAI models to power all of its tools. In order to use Marvin, you'll need an OpenAI API key.</p> <p>You can create an API key on the OpenAI platform. Once you've created it, set it as an environment variable called <code>OPENAI_API_KEY</code> (for any application on your machine to use) or <code>MARVIN_OPENAI_API_KEY</code> (if you only want Marvin to use it). In addition to setting it in your terminal, you can also write the variable to a dotenv file at <code>~/.marvin/.env</code>.</p> <p>For quick use, you can also pass your API key directly to Marvin at runtime. We do NOT recommend this for production:</p> <pre><code>import marvin\nmarvin.settings.openai.api_key = 'YOUR_API_KEY'\n</code></pre>"},{"location":"welcome/tutorial/#working-with-text","title":"Working with text","text":"<p>Marvin has a variety of tools that let you use LLMs to solve common but complex problems. In this tutorial, we'll try out a few of them to get a feel for how Marvin works. By the end of the tutorial, you'll have tried some of Marvin's advanced features and be ready to take on the universe! Just don't forget your towel.</p>"},{"location":"welcome/tutorial/#classification","title":"\ud83c\udff7\ufe0f Classification","text":"<p>Classification is one of Marvin's most straightforward features. Given some text and a list of labels, Marvin will choose the label that best fits the text. The <code>classify</code> function is great for tasks like sentiment analysis, intent classification, routing, and more.</p> <p>First steps: true/false</p> <p>Here is the simplest possible classification example, mapping the word \"yes\" to the boolean values <code>True</code> or <code>False</code>:</p> <pre><code>import marvin\n\nresult = marvin.classify(\"yes\", labels=bool)\n</code></pre> <p>Result</p> <pre><code>assert result is True\n</code></pre> <p>A more practical example: sentiment</p> <p>A more useful example is to classify text as one of several categories, provided as a list of labels. In this example, we build a basic sentiment classifier for any text:</p> <pre><code>import marvin\n\nresult = marvin.classify(\n    \"Marvin is so easy to use!\",\n    labels=[\"positive\", \"negative\", \"meh\"],\n)\n</code></pre> <p>Result</p> <pre><code>assert result == \"positive\"\n</code></pre> <p>This is a great example of how all Marvin tools should feel. Historically, classifying text was a major challenge for natural language processing frameworks. But with Marvin, it's as easy as calling a function.</p> <p>Structured labels</p> <p>For the <code>classify</code> function, you can supply labels as a <code>list</code> of labels, a <code>bool</code> type, an <code>Enum</code> class, or a <code>Literal</code> type. This gives you many options for returning structured (non-string) labels.</p>"},{"location":"welcome/tutorial/#transformation","title":"\ud83e\ude84 Transformation","text":"<p>Classification maps text to a single label, but what if you want to convert text to a more structured form? Marvin's <code>cast</code> function lets you do just that. Given some text and a target type, Marvin will return a structured representation of the text.</p> <p>Standardization</p> <p>Suppose you ran a survey and one of the questions asked where people live. Marvin can convert their freeform responses to a structured <code>Location</code> type:</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocation = marvin.cast(\"NYC\", target=Location)\n</code></pre> <p>Result</p> <p>The string \"NYC\" was converted to a full <code>Location</code> object:</p> <pre><code>assert location == Location(city=\"New York\", state=\"New York\")\n</code></pre>"},{"location":"welcome/tutorial/#instructions","title":"Instructions","text":"<p>All Marvin functions have an <code>instructions</code> parameter that let you fine-tune their behavior with natural language. For example, you can use instructions to tell Marvin to extract a specific type of information from a text or to format a response in a specific way.</p> <p>Suppose you wanted to standardize the survey responses in the previous example, but instead of using a full Pydantic model, you wanted the result to still be a string. The <code>cast</code> function will accept <code>target=str</code>, but that's so general it's unlikely to do what you want without additional guidance. That's where instructions come in:</p> <p>Instructions</p> <p>Repeat the previous example, but cast to a string according to the instructions:</p> <pre><code>import marvin\n\nlocation = marvin.cast(\n    \"NYC\", \n    target=str, \n    instructions=\"Return the proper city and state name\",\n)\n</code></pre> <p>Result</p> <p>The result is a string that complies with the instructions:</p> <pre><code>assert location == \"New York, New York\"\n</code></pre>"},{"location":"welcome/tutorial/#extraction","title":"\ud83d\udd0d Extraction","text":"<p>The <code>extract</code> function is like a generalization of the <code>cast</code> function: instead of transforming the entire text to a single target type, it extracts a list of entities from the text. This is useful for identifying people, places, ratings, keywords, and more.</p> <p>Feature extraction</p> <p>Suppose you wanted to extract the product features mentioned in a review:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions=\"extract product features\",\n)\n</code></pre> <p>Result</p> <pre><code>assert features == [\"camera\", \"battery life\"]\n</code></pre> <p>The <code>extract</code> function can take a target type, just like <code>cast</code>. This lets you extract structured entities from text. For example, you could extract a list of <code>Location</code> objects from a text:</p> <p>Location extraction</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.extract(\n    \"They've got a game in NY, then they go to DC before Los Angeles.\",\n    target=Location\n)\n</code></pre> <p>Result</p> <pre><code>assert locations == [\n    Location(city=\"New York\", state=\"New York\"),\n    Location(city=\"Washington\", state=\"District of Columbia\"),\n    Location(city=\"Los Angeles\", state=\"California\"),\n]\n</code></pre>"},{"location":"welcome/tutorial/#generation","title":"\u2728 Generation","text":"<p>So far, we've used Marvin to take existing text and convert it to a more structured or modified form that preserves its content but makes it easier to work with. Marvin can also generate synthetic data from a schema or instructions. This is incredibly useful for ideation, testing, data augmentation, populating databases, and more.</p> <p>Let's use Marvin's <code>generate</code> function to produce synthetic data. The <code>generate</code> function takes either a target type or natural language instructions (or both), as well as the number of items to generate, and returns a list of synthetic data that complies with the instructions.</p> <p>Locations named after presidents</p> <p>Earlier, we extracted locations from text. Now, let's generate some new locations:</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.generate(\n    n=4,\n    target=Location,\n    instructions=\"US cities named after presidents\",\n)\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>locations == [\n    Location(city=\"Washington\", state=\"DC\"),\n    Location(city=\"Jackson\", state=\"MS\"),\n    Location(city=\"Lincoln\", state=\"NE\"),\n    Location(city=\"Cleveland\", state=\"OH\"),\n]\n</code></pre> <p>In addition to structured types, Marvin can generate new text from instructions.</p> <p>Character names</p> <p>Let's generate some character names for a role-playing game:</p> <pre><code>import marvin\n\nnames = marvin.generate(\n    n=5,\n    instructions=\"Character names for a fantasy RPG\",\n)\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>names = [\n    \"Aelar Galanodel\",\n    \"Draka Steelshadow\",\n    \"Elyndra Silvershade\",\n    \"Brom Ironfist\",\n    \"Thalia Windwhisper\"\n]\n</code></pre>"},{"location":"welcome/tutorial/#ai-functions","title":"\ud83e\uddbe AI Functions","text":"<p>Now you've seen Marvin's most common tools. But what if you want to do something more custom? That's where AI functions come in. AI functions let you combine any inputs, instructions, and output types to create custom AI-powered behaviors.</p> <p>Marvin functions look just like regular Python functions, but notice that they don't have any source code. When you call these functions, the outputs are generated by an LLM on-demand. This means they can handle really complex tasks that even an LLM wouldn't know how to generate code for.</p> <p>Sentiment analysis</p> <p>Let's build a sentiment analysis function that takes text and returns a sentiment score. Normally, this would require a complex model and a lot of training data. But with Marvin, we only have to write the form of the function, and the AI takes care of the rest:</p> <pre><code>import marvin\n\n@marvin.fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Returns a sentiment score for `text` on a \n    scale of -1.0 (negative) to 1.0 (positive)\n    \"\"\"\n</code></pre> <p>Result</p> <p>Call the function to see its result:</p> <pre><code>sentiment(\"I love Marvin!\") # 0.8\nsentiment(\"This example could use some work...\") # -0.2\n</code></pre> <p>Marvin's AI functions are especially useful when you want to map a complex set of inputs to an output, usually involving some kind of natural language processing. For typed transformations or data generation, you may prefer to use a different Marvin tool, which have the real advantage of not looking \"odd\" to other Python developers. But for full control and conditional behaviors, AI functions are the way to go.</p>"},{"location":"welcome/tutorial/#working-with-images","title":"Working with images","text":"<p>Marvin is multi-modal! In addition to text, Marvin can also work with images. Most of Marvin's image and vision support is beta because it relies on the GPT-4 vision model, which is still in preview. But you wouldn't be here if you didn't love cutting-edge technology, right?</p>"},{"location":"welcome/tutorial/#generation_1","title":"\ud83c\udfa8 Generation","text":"<p>Marvin gives you easy access to the DALL-E 3 image generation model. This model can generate images from text descriptions.</p> <p>Generating images</p> <pre><code>import marvin\n\nmarvin.paint(\"a simple cup of coffee, still warm\")\n</code></pre> <p>Result</p> <p>( Note: your results may vary)</p> <p></p>"},{"location":"welcome/tutorial/#captioning","title":"\ud83d\udcdd Captioning","text":"<p>If you've already got an image, you can convert it to text using the <code>caption</code> function. Note the use of Marvin's <code>Image</code> type, which accepts either a local path to an image or a URL.</p> <p>Captioning images</p> <pre><code>import marvin\n\ncaption = marvin.beta.caption(marvin.beta.Image(\"path/to/coffee.png\"))\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>caption == \"\"\"\n    A ceramic cup of hot beverage with steam rising \n    from it, on a rustic wooden surface, backlit by \n    soft light coming in through a window.\n    \"\"\"\n</code></pre>"},{"location":"welcome/tutorial/#transformation-classification-and-extraction","title":"\ud83d\ude80  Transformation, classification, and extraction","text":"<p>Now that you've seen that Marvin can turn images into text, you're probably wondering if we can use that text with the <code>cast</code>, <code>extract</code>, and <code>classify</code> functions we saw earlier. The answer is yes -- but we can do even better.</p> <p>If you caption an image, the resulting text might not capture the details that are most relevant to the text processing task you want to perform. For example, if you want to classify the breed of dog in an image, you're going to need very specific information that a generic caption might not provide.</p> <p>Therefore, Marvin has beta versions of <code>cast</code>, <code>extract</code>, and <code>classify</code> that accept images as inputs. Instead of generating generic captions, these functions process the image in a way that is most conducive to the task at hand.</p> <p>These functions are available under <code>marvin.beta</code> and work identically to their text-only counterparts except that they can take images as well as text inputs.</p> <p>Identifying dog breeds in an image</p> <p>Let's identify the breed of each dog in this image by using the beta <code>extract</code> function.</p> <p></p> <pre><code>import marvin\n\nimg = marvin.beta.Image(\n    'https://images.unsplash.com/photo-1548199973-03cce0bbc87b',\n)\n\nresult = marvin.beta.extract(img, target=str, instructions='dog breeds')\n</code></pre> <p>Result</p> <pre><code>result == ['Pembroke Welsh Corgi', 'Yorkshire Terrier']\n</code></pre>"},{"location":"welcome/tutorial/#grab-your-towel","title":"Grab your towel","text":"<p>We hope this tutorial has given you a taste of what Marvin can do. There's a lot more to explore, including tools for interactive use cases (like chatbots and applications), audio generation, and more.</p> <p>To learn more, please explore the docs or say hi in our Discord community!</p> <p>And remember:</p> <p>Don't panic!</p> <pre><code>import marvin\n\naudio = marvin.speak(\"and above all else... don't panic!\")\naudio.stream_to_file(\"dont_panic.mp3\")\n</code></pre> <p>Result</p> <p>      Your browser does not support the audio element. </p>"},{"location":"welcome/what_is_marvin/","title":"What is Marvin?","text":"<p>Marvin is a lightweight AI toolkit for building natural language interfaces that are reliable, scalable, and easy to trust.</p> <p>Each of Marvin's tools is simple and self-documenting, using AI to solve common but complex challenges like entity extraction, classification, and generating synthetic data. Each tool is independent and incrementally adoptable, so you can use them on their own or in combination with any other library. Marvin is also multi-modal, supporting both image and audio generation as well using images as inputs for extraction and classification.</p> <p>Marvin is for developers who care more about using AI than building AI, and we are focused on creating an exceptional developer experience. Marvin users should feel empowered to bring tightly-scoped \"AI magic\" into any traditional software project with just a few extra lines of code.</p> <p>Marvin aims to merge the best practices for building dependable, observable software with the best practices for building with generative AI into a single, easy-to-use library. It's a serious tool, but we hope you have fun with it.</p> <p>Marvin is open-source, free to use, and made with \ud83d\udc99 by the team at Prefect.</p> Explain Marvin like I'm 5 (I'm a technical 5-year-old)(I'm not technical) <p>Marvin lets your software speak English and ask questions to Large Language Models.</p> <p>It introspects the types and docstrings of your functions and data models, and automatically renders them as prompts for an LLM. You write code as you would normally, rather than prompts, and Marvin handles the back-and-forth translation.</p> <p>This lets you focus on what you've always focused on: writing clean, versioned, reusable code and data models, and not scrutinizing whether you begged your LLM hard enough to output JSON or needed to offer it a bigger tip for the right answer.</p> <p>Extracting, generating, cleaning, or classifying data is as simple as writing a function or a data model.</p> <p>Marvin lets engineers who know Python use Large Language Models without needing to write prompts.</p> <p>It turns out that ChatGPT and other Large Language Models are good at performing boring but incredibly valuable business-critical tasks beyond being a chatbot: you can use them to classify emails as spam, extract key figures from a report - exactly however you want for your scenario. When you use something like ChatGPT you spend a lot of time crafting the right prompt or context to get it to write your email, plan your date night, etc.</p> <p>If you want your software to use ChatGPT, you need to let it turn its objective into English. Marvin handles this 'translation' for you, so you get to just write code like you normally would. Engineers like using Marvin because it lets them write software like they're used to.</p> <p>Simply put, it lets you use Generative AI without feeling like you have to learn a framework.</p> <p>What's it like to use Marvin?</p> Classify textExtract entitiesStructure text and imagesGenerate dataCustom AI functions <p>Classify text using a set of labels:</p> <pre><code>import marvin\n\nresult = marvin.classify(\n    \"Marvin is so easy to use!\",\n    labels=[\"positive\", \"negative\"],\n)\n</code></pre> <p>Result</p> <pre><code>assert result == \"positive\"\n</code></pre> <p>Extract product features from user feedback:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions=\"product features\",\n)\n</code></pre> <p>Result</p> <pre><code>features == ['camera', 'battery life']\n</code></pre> <p>Marvin can convert natural language to a structured form:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description='2-letter abbreviation')\n\n\nresult = marvin.cast('the big apple', Location)\n</code></pre> <p>Result</p> <pre><code>assert result == Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>Marvin is multimodal, with beta support for using images as inputs. As an example, let's compare this photo to a shopping list to generate a list of missing items:</p> <p></p> <pre><code>import marvin\n\nshopping_list = [\"bagels\", \"cabbage\", \"eggs\", \"apples\", \"oranges\"]\ngroceries = marvin.beta.Image(\n    \"https://images.unsplash.com/photo-1588964895597-cfccd6e2dbf9\",\n)\n\nmissing_items = marvin.beta.cast(\n    groceries, \n    target=list[str], \n    instructions=f\"Did I forget anything on my list: {shopping_list}?\",\n)\n</code></pre> <p>Result</p> <pre><code>assert missing_items == [\"eggs\", \"oranges\"]\n</code></pre> <p>Generate synthetic data from a schema and instructions:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description='2-letter abbreviation')\n\n\nresult = marvin.generate(\n    n=4,\n    target=Location,\n    instructions='US cities named after presidents',\n)\n</code></pre> <p>Result</p> <pre><code>result == [\n    Location(city=\"Washington\", state=\"DC\"),\n    Location(city=\"Jefferson City\", state=\"MO\"),\n    Location(city=\"Lincoln\", state=\"NE\"),\n    Location(city=\"Madison\", state=\"WI\"),\n]\n</code></pre> <p>Marvin functions let you combine any inputs, instructions, and output types to create custom AI-powered behaviors.</p> <pre><code>import marvin\n\n\n@marvin.fn\ndef list_fruits(n: int, color: str) -&gt; list[str]:\n    \"\"\"Generates a list of `n` fruits that are `color`\"\"\"\n\n\nfruits = list_fruits(3, color='red')\n</code></pre> <p>Result</p> <pre><code>fruits == [\"apple\", \"cherry\", \"strawberry\"]\n</code></pre> <p>Note that <code>list_fruits</code> has no source code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p>"}]}